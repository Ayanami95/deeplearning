{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_binary.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/chokkan/deeplearning/blob/master/notebook/binary.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "CX_9BuPB_hA-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feedforward Neural Networks\n",
        "\n",
        "This Jupyter notebook explains various ways of implementing single-layer and multi-layer neural networks. The implementations are arranged by concrete (explicit) to abstract order so that one can understand the black-boxed processing in deep learning frameworks.\n",
        "\n",
        "In order to focus on understanding the internals of training, this notebook uses a simple and classic example: *threshold logic units*.\n",
        "Supposing $x=0$ as *false* and $x=1$ as *true*, single-layer neural networks can realize logic units such as AND ($\\wedge$), OR ($\\vee$), NOT ($\\lnot$), and NAND ($|$). Multi-layer neural networks can realize logical compounds such as XOR.\n",
        "\n",
        "| $x_1$ | $x_2$ | AND | OR | NAND | XOR |\n",
        "| :---: |:-----:|:---:|:--:|:----:|:---:|\n",
        "| 0 | 0 | 0 | 0 | 1 | 0 |\n",
        "| 0 | 1 | 0 | 1 | 1 | 1 |\n",
        "| 1 | 0 | 0 | 1 | 1 | 1 |\n",
        "| 1 | 1 | 1 | 1 | 0 | 0 |\n"
      ]
    },
    {
      "metadata": {
        "id": "qxdFG8U2Net8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using numpy"
      ]
    },
    {
      "metadata": {
        "id": "Ea5cM4JEsENr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer perceptron\n",
        "\n",
        "A single layer perceptron predicts a binary label $\\hat{y} \\in \\{0, 1\\}$ for a given input vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ ($d$ presents the number of dimensions of inputs) by using the following formula,\n",
        "$$\n",
        "\\hat{y} = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = g(w_1 x_1 + w_2 x_2 + ... + w_d x_d + b)\n",
        "$$\n",
        "\n",
        "Here, $\\boldsymbol{w} \\in \\mathbb{R}^d$ is a weight vector; $b \\in \\mathbb{R}$ is a bias weight; and $g(.)$ denotes a Heaviside step function (we assume $g(0)=0$).\n",
        "\n",
        "Let's train a NAND gate with two inputs ($d = 2$). More specifically, we want to find a weight vector $\\boldsymbol{w}$ and a bias weight $b$ of a single-layer perceptron that realizes the truth table of the NAND gate: $\\{0,1\\}^2 \\to \\{0,1\\}$.\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$  |\n",
        "| :---: |:-----:|:----:|\n",
        "| 0 | 0 | 1|\n",
        "| 0 | 1 | 1|\n",
        "| 1 | 0 | 1|\n",
        "| 1 | 1 | 0|\n",
        "\n",
        "We convert the truth table into a training set consisting of all mappings of the NAND gate,\n",
        "$$\n",
        "\\boldsymbol{x}_1 = (0, 0), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}_2 = (0, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}_3 = (1, 0), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}_4 = (1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "In order to train a weight vector and bias weight in a unified code, we include a bias term as an additional dimension to inputs. More concretely, we append $1$ to each input,\n",
        "$$\n",
        "\\boldsymbol{x}'_1 = (0, 0, 1), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}'_2 = (0, 1, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}'_3 = (1, 0, 1), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}'_4 = (1, 1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "Then, the formula of the single-layer perceptron becomes,\n",
        "$$\n",
        "\\hat{y} = g((w_1, w_2, w_3) \\cdot \\boldsymbol{x}') = g(w_1 x_1 + w_2 x_2 + w_3)\n",
        "$$\n",
        "In other words, $w_1$ and $w_2$ present weights for $x_1$ and $x_2$, respectively, and $w_3$ does a bias weight.\n",
        "\n",
        "The code below implements Rosenblatt's perceptron algorithm with a fixed number of iterations (100 times). We use a constant learning rate 0.5 for simplicity.\n"
      ]
    },
    {
      "metadata": {
        "id": "2ygoUjQYrPoj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    for i in range(len(y)):\n",
        "        y_pred = np.heaviside(np.dot(x[i], w), 0)\n",
        "        w += (y[i] - y_pred) * eta * x[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYoeshu2rXdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5795355a-ce1b-4c7b-dfc9-b7cfcd9ad6c2"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1. ,  0.5, -1. ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "hOFgUFojraFA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31676a50-61d6-4581-f972-90b1144df906"
      },
      "cell_type": "code",
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "bl_ZAEguNzgU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer perceptron with mini-batch\n",
        "\n",
        "It is desireable to reduce the execusion run by the Python interpreter, which is relatively slow. The common technique to speed up a machine-learning code written in Python is to to execute computations within the matrix library (e.g., numpy).\n",
        "\n",
        "The single-layer perceptron makes predictions for four inputs,\n",
        "$$\n",
        "\\hat{y}_1 = g(\\boldsymbol{x}_1 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_2 = g(\\boldsymbol{x}_2 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_3 = g(\\boldsymbol{x}_3 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_4 = g(\\boldsymbol{x}_4 \\cdot \\boldsymbol{w}) \\\\\n",
        "$$\n",
        "\n",
        "Here, we define $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ and $X \\in \\mathbb{R}^{4 \\times d}$ as,\n",
        "$$\n",
        "\\hat{Y} = \\begin{pmatrix} \n",
        "  \\hat{y}_1 \\\\ \n",
        "  \\hat{y}_2 \\\\ \n",
        "  \\hat{y}_3 \\\\ \n",
        "  \\hat{y}_4 \\\\ \n",
        "\\end{pmatrix},\n",
        "X = \\begin{pmatrix} \n",
        "  \\boldsymbol{x}_1 \\\\ \n",
        "  \\boldsymbol{x}_2 \\\\ \n",
        "  \\boldsymbol{x}_3 \\\\ \n",
        "  \\boldsymbol{x}_4 \\\\ \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Then, we can write the four predictions in one dot-product computation,\n",
        "$$\n",
        "\\hat{Y} = X \\cdot \\boldsymbol{w}\n",
        "$$\n",
        "\n",
        "The code below implements this idea. The function `np.heaviside()` yields a vector corresponding to the four predictions, applying the step function for every element of the argument.\n",
        "\n",
        "This technique is frequently used in mini-batch training."
      ]
    },
    {
      "metadata": {
        "id": "2fK-_WimtPwb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = np.heaviside(np.dot(x, w), 0)\n",
        "    w += np.dot((y - y_pred), x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_x4p1BldtQ-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f01a29f7-d0ec-4e12-9f1d-9ffe6ad99fe3"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1., -1.,  2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "E-P2RpWrtVyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebc82302-8cc1-4a16-9515-c0970f91c1a5"
      },
      "cell_type": "code",
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "nkxBcCSTtDvm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient descent (SGD) with mini-batch"
      ]
    },
    {
      "metadata": {
        "id": "bltpfNRctjV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(v):\n",
        "    return 1.0 / (1 + np.exp(-v))\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = sigmoid(np.dot(x, w))\n",
        "    w -= np.dot((y_pred - y), x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9bASDMfhtm-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eeb3bf87-4a94-4189-c0f4-f3ebdb5005a7"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.59504346, -5.59504346,  8.57206068])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "-69r0c4KtqlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "65263ac7-b37e-496b-c651-47de7e3b5b8f"
      },
      "cell_type": "code",
      "source": [
        "sigmoid(np.dot(x, w))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.99981071, 0.95152498, 0.95152498, 0.06798725])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "ELUeNFRFuJv3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Automatic differentiation"
      ]
    },
    {
      "metadata": {
        "id": "aB0DwVOyuXP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using autograd\n",
        "\n",
        "Installing [autograd](https://github.com/HIPS/autograd) (do this once)."
      ]
    },
    {
      "metadata": {
        "id": "kpt7DJ7a-qov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5d0d3f7b-8852-4dc3-dc74-1e9768595df3"
      },
      "cell_type": "code",
      "source": [
        "!pip install autograd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autograd in /usr/local/lib/python3.6/dist-packages (1.2)\r\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\r\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.5)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DyC9iOFO-1cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "89aa3a6a-f865-477a-f4d9-c6f4f0a572f9"
      },
      "cell_type": "code",
      "source": [
        "import autograd\n",
        "import autograd.numpy as np\n",
        "\n",
        "def loss(w, x):\n",
        "    return -np.log(1.0 / (1 + np.exp(-np.dot(x, w))))\n",
        "\n",
        "x = np.array([1, 1, 1])\n",
        "w = np.array([1.0, 1.0, -1.5])\n",
        "\n",
        "grad_loss = autograd.grad(loss)\n",
        "print(loss(w, x))\n",
        "print(grad_loss(w, x))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47407698418010663\n",
            "[-0.37754067 -0.37754067 -0.37754067]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "59D2aARTuQDL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using pytorch"
      ]
    },
    {
      "metadata": {
        "id": "VOoZXvXP-6b2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "d184e47f-ec66-4c78-e777-d928ac22e5a5"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.0)\r\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\r\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nHixTUut_LIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "30d28589-4e9f-451c-ab36-d76db7887149"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "x = torch.tensor([1, 1, 1], dtype=dtype)\n",
        "w = torch.tensor([1.0, 1.0, -1.5], dtype=dtype, requires_grad=True)\n",
        "\n",
        "loss = -torch.dot(x, w).sigmoid().log()\n",
        "loss.backward()\n",
        "print(loss.item())\n",
        "print(w.grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4740769565105438\n",
            "tensor([-0.3775, -0.3775, -0.3775])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VFzuau5gu4vY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing neural networks with pytorch"
      ]
    },
    {
      "metadata": {
        "id": "STwWdvJCva4G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer neural network using automatic differentiation"
      ]
    },
    {
      "metadata": {
        "id": "xOJHDGYKIgsm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "w = torch.randn(3, 1, dtype=dtype, requires_grad=True)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    # y_pred = \\sigma(x \\cdot w)\n",
        "    y_pred = x.mm(w).sigmoid()\n",
        "    ll = y * y_pred + (1 - y) * (1 - y_pred)\n",
        "    loss = -ll.log().sum()      # The loss value.\n",
        "    #print(t, loss.item())\n",
        "    loss.backward()             # Compute the gradients of the loss.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w -= eta * w.grad       # Update weights using SGD.        \n",
        "        w.grad.zero_()          # Clear the gradients for the next iteration."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FyoS5iP6n7Ru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "fe6de935-5f28-4a67-9f97-b8447e138f2b"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.2228],\n",
              "        [-4.2234],\n",
              "        [ 6.5268]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Jfp604gFn9Yw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "27fdd3f8-8607-4076-a962-0ac38618f8d8"
      },
      "cell_type": "code",
      "source": [
        "x.mm(w).sigmoid()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9985],\n",
              "        [ 0.9092],\n",
              "        [ 0.9092],\n",
              "        [ 0.1279]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "w6eu-AuDvl9J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-layer neural network using automatic differentiation"
      ]
    },
    {
      "metadata": {
        "id": "ts2RTKVPn_xk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "w1 = torch.randn(3, 2, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(2, 1, dtype=dtype, requires_grad=True)\n",
        "b2 = torch.randn(1, 1, dtype=dtype, requires_grad=True)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(1000):\n",
        "    # y_pred = \\sigma(w_2 \\cdot \\sigma(x \\cdot w_1) + b_2)\n",
        "    y_pred = x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()\n",
        "    ll = y * y_pred + (1 - y) * (1 - y_pred)\n",
        "    loss = -ll.log().sum()\n",
        "    #print(t, loss.item())\n",
        "    loss.backward()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Update weights using SGD.\n",
        "        w1 -= eta * w1.grad\n",
        "        w2 -= eta * w2.grad\n",
        "        b2 -= eta * b2.grad\n",
        "        \n",
        "        # Clear the gradients for the next iteration.\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()\n",
        "        b2.grad.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWgbqAXawEof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "c1b87740-d7b4-4ba3-f327-81bf56199850"
      },
      "cell_type": "code",
      "source": [
        "print(w1)\n",
        "print(w2)\n",
        "print(b2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-5.4129, -7.0510],\n",
            "        [-5.4112, -7.0391],\n",
            "        [ 8.0656,  2.9682]])\n",
            "tensor([[ 11.6259],\n",
            "        [-12.0312]])\n",
            "tensor([[-5.4634]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yS4bql3foxB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "dadf8c7c-429e-426a-d9f4-53bf8e0fa4c6"
      },
      "cell_type": "code",
      "source": [
        "x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0050],\n",
              "        [ 0.9945],\n",
              "        [ 0.9945],\n",
              "        [ 0.0084]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "OGS23bDSazMJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer neural network with high-level NN modules"
      ]
    },
    {
      "metadata": {
        "id": "Jt9eizLFo1iN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = model(x)                   # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    model.zero_grad()                   # Zero-clear the gradients.\n",
        "    loss.backward()                     # Compute the gradients.\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= eta * param.grad   # Update the parameters using SGD."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zq6oqLmIENFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "83ddfa07-461e-4746-8669-6a74d6bdd3d8"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-4.2632, -4.2634]])),\n",
              "             ('0.bias', tensor([ 6.5866]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "QdPOdNgO840b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "afee61a6-608d-4ba0-9cac-e09947421f8c"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9986],\n",
              "        [ 0.9108],\n",
              "        [ 0.9108],\n",
              "        [ 0.1256]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "KfuoJMeqbClA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-layer neural network with high-level NN modules"
      ]
    },
    {
      "metadata": {
        "id": "D6ss25zA9nPk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n",
        "    torch.nn.Sigmoid(),                 # Sigmoid function\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)                   # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    model.zero_grad()                   # Zero-clear the gradients.\n",
        "    loss.backward()                     # Compute the gradients.\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= eta * param.grad   # Update the parameters using SGD."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iLFGZWL0--2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "0f2c7a0f-c31e-486b-994b-8d91f20e0835"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-5.3436, -5.3484],\n",
              "                      [-6.9310, -6.9626]])),\n",
              "             ('0.bias', tensor([ 7.9648,  2.9143])),\n",
              "             ('2.weight', tensor([[ 11.4116, -11.8373]])),\n",
              "             ('2.bias', tensor([-5.3505]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "1BF6-W_J-82M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f6d54774-9eb7-49eb-a6cc-8fd3fc8cae4a"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0056],\n",
              "        [ 0.9938],\n",
              "        [ 0.9938],\n",
              "        [ 0.0095]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "gGK3DJBubb0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer neural network with an optimizer."
      ]
    },
    {
      "metadata": {
        "id": "puqbP4F9bidv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(100):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RBUX1BUhcDK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "99f6202e-af36-49b8-85e9-aca6d08d13b3"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-4.2689, -4.2692]])),\n",
              "             ('0.bias', tensor([ 6.5951]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "nJWcDaPpcKCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "109c780d-03a7-4dfe-aa75-30aa619e05e5"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9986],\n",
              "        [ 0.9110],\n",
              "        [ 0.9110],\n",
              "        [ 0.1253]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "scIE8zZWdhLs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-layer neural networks using an optimizer"
      ]
    },
    {
      "metadata": {
        "id": "J1BrWIxkcMfI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n",
        "    torch.nn.Sigmoid(),                 # Sigmoid function\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mp0sNnxhducs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "eb32ebb1-c216-4858-ad49-405349d6e4e3"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-5.8059, -5.8059],\n",
              "                      [ 7.3491,  7.3492]])),\n",
              "             ('0.bias', tensor([ 8.6991, -3.3325])),\n",
              "             ('2.weight', tensor([[ 9.7696,  9.6699]])),\n",
              "             ('2.bias', tensor([-14.2857]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "AYgEteqydwt2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "91fab3bd-da9a-4e28-d2d1-d5244ca4eadb"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0150],\n",
              "        [ 0.9887],\n",
              "        [ 0.9887],\n",
              "        [ 0.0161]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "7x7Ed3SBGjPx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer neural network with a customizable NN class."
      ]
    },
    {
      "metadata": {
        "id": "klKcvlpVkpk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network model.\n",
        "class SingleLayerNN(torch.nn.Module):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super(SingleLayerNN, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(d_in, d_out, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear1(x)\n",
        "\n",
        "model = SingleLayerNN(2, 1)\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(100):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ExxVfDnB5vPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "646b257f-8f9b-4a7b-b5e4-398135dce060"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight', tensor([[-4.3039, -4.3036]])),\n",
              "             ('linear1.bias', tensor([ 6.6467]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "xKczD7tZ518W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "b6892294-be8e-4148-deae-98ce2544cc3d"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9987],\n",
              "        [ 0.9124],\n",
              "        [ 0.9124],\n",
              "        [ 0.1234]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "jzTPZXUxGNso",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-layer neural network with a customizable NN class.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gSpf1qft53-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network model.\n",
        "class ThreeLayerNN(torch.nn.Module):\n",
        "    def __init__(self, d_in, d_hidden, d_out):\n",
        "        super(ThreeLayerNN, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(d_in, d_hidden, bias=True)\n",
        "        self.linear2 = torch.nn.Linear(d_hidden, d_out, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.linear1(x).sigmoid())\n",
        "\n",
        "model = ThreeLayerNN(2, 2, 1)\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k6LLbCf0684G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "f7ae4123-1f79-467e-bb46-6635ba0ea628"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight', tensor([[-6.7532,  6.5182],\n",
              "                      [ 6.3477, -6.6279]])),\n",
              "             ('linear1.bias', tensor([-3.5757, -3.4740])),\n",
              "             ('linear2.weight', tensor([[ 11.2248,  11.2577]])),\n",
              "             ('linear2.bias', tensor([-5.5168]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "iPhm3J4R61S2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "7fcda9bc-ac68-45bd-bbb6-f2193e383451"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0076],\n",
              "        [ 0.9942],\n",
              "        [ 0.9942],\n",
              "        [ 0.0066]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "OeOa6mt5Fgw4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}