{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_binary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chokkan/deeplearning/blob/master/notebook/binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CX_9BuPB_hA-"
      },
      "source": [
        "# Feedforward Neural Networks\n",
        "\n",
        "This Jupyter notebook explains various ways of implementing single-layer and multi-layer neural networks. The implementations are arranged by concrete (explicit) to abstract order so that one can understand the black-boxed processing in deep learning frameworks.\n",
        "\n",
        "In order to focus on understanding the internals of training, this notebook uses a simple and classic example: *threshold logic units*.\n",
        "Supposing $x=0$ as *false* and $x=1$ as *true*, single-layer neural networks can realize logic units such as AND ($\\wedge$), OR ($\\vee$), NOT ($\\lnot$), and NAND ($|$). Multi-layer neural networks can realize logical compounds such as XOR.\n",
        "\n",
        "| $x_1$ | $x_2$ | AND | OR | NAND | XOR |\n",
        "| :---: |:-----:|:---:|:--:|:----:|:---:|\n",
        "| 0 | 0 | 0 | 0 | 1 | 0 |\n",
        "| 0 | 1 | 0 | 1 | 1 | 1 |\n",
        "| 1 | 0 | 0 | 1 | 1 | 1 |\n",
        "| 1 | 1 | 1 | 1 | 0 | 0 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qxdFG8U2Net8"
      },
      "source": [
        "## Using numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug4zqMhLB-B6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ea5cM4JEsENr"
      },
      "source": [
        "### Single-layer perceptron\n",
        "\n",
        "A single layer perceptron predicts a binary label $\\hat{y} \\in \\{0, 1\\}$ for a given input vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ ($d$ presents the number of dimensions of inputs) by using the following formula,\n",
        "$$\n",
        "\\hat{y} = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = g(w_1 x_1 + w_2 x_2 + ... + w_d x_d + b)\n",
        "$$\n",
        "\n",
        "Here, $\\boldsymbol{w} \\in \\mathbb{R}^d$ is a weight vector; $b \\in \\mathbb{R}$ is a bias weight; and $g(.)$ denotes a Heaviside step function (we assume $g(0)=0$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRpaNDA8BWJY",
        "colab_type": "text"
      },
      "source": [
        "For simplicity, let us consider examples with two-dimensional inputs ($d=2$).\n",
        "We can represent an input vector $\\boldsymbol{x} \\in \\mathbb{R}^2$ and weight vector $\\boldsymbol{w} \\in \\mathbb{R}^2$ with `numpy.array`. We also define the bias term $b$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyX1MfRvBD25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([0, 1])\n",
        "w = np.array([1.0, 1.0])\n",
        "b = 1.0"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-P2AkbTCI0P",
        "colab_type": "text"
      },
      "source": [
        "The following code computes $\\boldsymbol{w} \\cdot \\boldsymbol{x} + b$,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g_115nLCGqs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4a76f3f-8d84-46c3-f12c-06e3fb22a427"
      },
      "source": [
        "np.dot(x, w) + b"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUzhy8YFDIuf",
        "colab_type": "text"
      },
      "source": [
        "Applying Heaviside step function $g$ to the result yields a binary label $\\hat{y}$,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV6a5JT5DH6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4610f800-9924-4abc-cf39-a4e98c2e12b8"
      },
      "source": [
        "np.heaviside(np.dot(x, w) + b, 0)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ppo_Jh8EEs3",
        "colab_type": "text"
      },
      "source": [
        "#### Including the bias term into the weight vector\n",
        "\n",
        "For the simplicity of implementations, we include a bias term `b` as an additional dimension to the weight vector `w`. More concretely, we append an element with the value of $1$ to each input,\n",
        "$$\n",
        "\\boldsymbol{x} = (0, 1) \\rightarrow \\boldsymbol{x}' = (0, 1, 1)\n",
        "$$\n",
        "and expand the dimension of the weight vector $\\boldsymbol{w} \\in \\mathbb{R}^{3}$.\n",
        "\n",
        "Then, the formula of the single-layer perceptron becomes,\n",
        "$$\n",
        "\\hat{y} = g((w_1, w_2, w_3) \\cdot \\boldsymbol{x}') = g(w_1 x_1 + w_2 x_2 + w_3)\n",
        "$$\n",
        "In other words, $w_1$ and $w_2$ present weights for $x_1$ and $x_2$, respectively, and $w_3$ does a bias weight."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmQGDVM2FnGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([0, 1, 1])\n",
        "w = np.array([1.0, 1.0, 1.0])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53zn1dWQFvLw",
        "colab_type": "text"
      },
      "source": [
        "We can simplify the code to predict a binary label $\\hat{y}$,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsgq_oOzF4PZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2af1b7a-c562-442d-e3ac-fa5599709e94"
      },
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53SDTFENBA19",
        "colab_type": "text"
      },
      "source": [
        "#### Training a NAND gate\n",
        "\n",
        "Let's train a NAND gate with two inputs. More specifically, we want to find a weight vector $\\boldsymbol{w}$ and a bias weight $b$ of a single-layer perceptron that realizes the truth table of the NAND gate: $\\{0,1\\}^2 \\to \\{0,1\\}$.\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$  |\n",
        "| :---: |:-----:|:----:|\n",
        "| 0 | 0 | 1|\n",
        "| 0 | 1 | 1|\n",
        "| 1 | 0 | 1|\n",
        "| 1 | 1 | 0|\n",
        "\n",
        "We convert the truth table into a training set consisting of all mappings of the NAND gate,\n",
        "$$\n",
        "\\boldsymbol{x}_1 = (0, 0), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}_2 = (0, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}_3 = (1, 0), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}_4 = (1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "As explained earlier, we include the bias term into the last dimension.\n",
        "$$\n",
        "\\boldsymbol{x}'_1 = (0, 0, 1), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}'_2 = (0, 1, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}'_3 = (1, 0, 1), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}'_4 = (1, 1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "The code below implements Rosenblatt's perceptron algorithm with a fixed number of iterations (50 times). We use a constant learning rate 0.5 for simplicity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ygoUjQYrPoj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "a80eee1a-6b3c-47bf-959c-2c3b0f821f1c"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(50):\n",
        "    # Pick an instance index (i) at random.\n",
        "    i = random.choice(range(len(y)))\n",
        "    # Predict the label for the instance x[i] with the current parameter w.\n",
        "    y_pred = np.heaviside(np.dot(x[i], w), 0)\n",
        "    # Show the detail of the instance and the current parameter.\n",
        "    print(f'#{t}: i={i}, x={x[i]}, w={w}, y={y[i]}, y_pred={y_pred}, y_err={y[i] - y_pred}')\n",
        "    # Update the parameter.\n",
        "    w += (y[i] - y_pred) * eta * x[i]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0: i=1, x=[0 1 1], w=[0. 0. 0.], y=0, y_pred=0.0, y_err=0.0\n",
            "#1: i=2, x=[1 0 1], w=[0. 0. 0.], y=0, y_pred=0.0, y_err=0.0\n",
            "#2: i=2, x=[1 0 1], w=[0. 0. 0.], y=0, y_pred=0.0, y_err=0.0\n",
            "#3: i=2, x=[1 0 1], w=[0. 0. 0.], y=0, y_pred=0.0, y_err=0.0\n",
            "#4: i=3, x=[1 1 1], w=[0. 0. 0.], y=1, y_pred=0.0, y_err=1.0\n",
            "#5: i=2, x=[1 0 1], w=[0.5 0.5 0.5], y=0, y_pred=1.0, y_err=-1.0\n",
            "#6: i=1, x=[0 1 1], w=[0.  0.5 0. ], y=0, y_pred=1.0, y_err=-1.0\n",
            "#7: i=2, x=[1 0 1], w=[ 0.   0.  -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#8: i=3, x=[1 1 1], w=[ 0.   0.  -0.5], y=1, y_pred=0.0, y_err=1.0\n",
            "#9: i=3, x=[1 1 1], w=[0.5 0.5 0. ], y=1, y_pred=1.0, y_err=0.0\n",
            "#10: i=3, x=[1 1 1], w=[0.5 0.5 0. ], y=1, y_pred=1.0, y_err=0.0\n",
            "#11: i=3, x=[1 1 1], w=[0.5 0.5 0. ], y=1, y_pred=1.0, y_err=0.0\n",
            "#12: i=0, x=[0 0 1], w=[0.5 0.5 0. ], y=0, y_pred=0.0, y_err=0.0\n",
            "#13: i=1, x=[0 1 1], w=[0.5 0.5 0. ], y=0, y_pred=1.0, y_err=-1.0\n",
            "#14: i=3, x=[1 1 1], w=[ 0.5  0.  -0.5], y=1, y_pred=0.0, y_err=1.0\n",
            "#15: i=2, x=[1 0 1], w=[1.  0.5 0. ], y=0, y_pred=1.0, y_err=-1.0\n",
            "#16: i=3, x=[1 1 1], w=[ 0.5  0.5 -0.5], y=1, y_pred=1.0, y_err=0.0\n",
            "#17: i=3, x=[1 1 1], w=[ 0.5  0.5 -0.5], y=1, y_pred=1.0, y_err=0.0\n",
            "#18: i=2, x=[1 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#19: i=1, x=[0 1 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#20: i=3, x=[1 1 1], w=[ 0.5  0.5 -0.5], y=1, y_pred=1.0, y_err=0.0\n",
            "#21: i=3, x=[1 1 1], w=[ 0.5  0.5 -0.5], y=1, y_pred=1.0, y_err=0.0\n",
            "#22: i=2, x=[1 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#23: i=1, x=[0 1 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#24: i=2, x=[1 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#25: i=1, x=[0 1 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#26: i=2, x=[1 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#27: i=2, x=[1 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#28: i=1, x=[0 1 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#29: i=0, x=[0 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#30: i=1, x=[0 1 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#31: i=0, x=[0 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#32: i=3, x=[1 1 1], w=[ 0.5  0.5 -0.5], y=1, y_pred=1.0, y_err=0.0\n",
            "#33: i=2, x=[1 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#34: i=3, x=[1 1 1], w=[ 0.5  0.5 -0.5], y=1, y_pred=1.0, y_err=0.0\n",
            "#35: i=1, x=[0 1 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#36: i=0, x=[0 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#37: i=3, x=[1 1 1], w=[ 0.5  0.5 -0.5], y=1, y_pred=1.0, y_err=0.0\n",
            "#38: i=2, x=[1 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#39: i=0, x=[0 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#40: i=0, x=[0 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#41: i=2, x=[1 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#42: i=0, x=[0 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#43: i=2, x=[1 0 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#44: i=3, x=[1 1 1], w=[ 0.5  0.5 -0.5], y=1, y_pred=1.0, y_err=0.0\n",
            "#45: i=1, x=[0 1 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#46: i=1, x=[0 1 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#47: i=3, x=[1 1 1], w=[ 0.5  0.5 -0.5], y=1, y_pred=1.0, y_err=0.0\n",
            "#48: i=1, x=[0 1 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n",
            "#49: i=1, x=[0 1 1], w=[ 0.5  0.5 -0.5], y=0, y_pred=0.0, y_err=0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8geTVVpnlu1O",
        "colab_type": "text"
      },
      "source": [
        "We can confirm the learned parameter and classification results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TYoeshu2rXdK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ded75cf6-3d04-41be-86ab-2b11c5f711d7"
      },
      "source": [
        "w"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.5,  0.5, -0.5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hOFgUFojraFA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb1e545b-e248-4bf8-92ad-557b816a8152"
      },
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bl_ZAEguNzgU"
      },
      "source": [
        "### Single-layer perceptron with mini-batch\n",
        "\n",
        "It is desireable to reduce the execusion run by the Python interpreter, which is extremely slow. The common technique to speed up a machine-learning code written in Python is to to execute computations within the matrix library (e.g., numpy).\n",
        "\n",
        "The single-layer perceptron makes predictions for four inputs,\n",
        "$$\n",
        "\\hat{y}_1 = g(\\boldsymbol{x}_1 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_2 = g(\\boldsymbol{x}_2 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_3 = g(\\boldsymbol{x}_3 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_4 = g(\\boldsymbol{x}_4 \\cdot \\boldsymbol{w}) \\\\\n",
        "$$\n",
        "\n",
        "Here, we define $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ and $X \\in \\mathbb{R}^{4 \\times d}$ as,\n",
        "$$\n",
        "\\hat{Y} = \\begin{pmatrix} \n",
        "  \\hat{y}_1 \\\\ \n",
        "  \\hat{y}_2 \\\\ \n",
        "  \\hat{y}_3 \\\\ \n",
        "  \\hat{y}_4 \\\\ \n",
        "\\end{pmatrix},\n",
        "X = \\begin{pmatrix} \n",
        "  \\boldsymbol{x}_1 \\\\ \n",
        "  \\boldsymbol{x}_2 \\\\ \n",
        "  \\boldsymbol{x}_3 \\\\ \n",
        "  \\boldsymbol{x}_4 \\\\ \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Then, we can write the four predictions in one dot-product computation,\n",
        "$$\n",
        "\\hat{Y} = X \\cdot \\boldsymbol{w}\n",
        "$$\n",
        "\n",
        "The code below implements this idea. The function `np.heaviside()` yields a vector corresponding to the four predictions, applying the step function for every element of the argument.\n",
        "\n",
        "This technique is frequently used in mini-batch training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2fK-_WimtPwb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b66824aa-43c7-496f-a00d-355aaab6d770"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(10):\n",
        "    y_pred = np.heaviside(np.dot(x, w), 0)\n",
        "    print(f'#{t}: w={w}, Y={y}, Ypred={y_pred}, Yerr={y-y_pred}, dw={np.dot((y - y_pred), x)}')\n",
        "    w += np.dot((y - y_pred), x)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0: w=[0. 0. 0.], Y=[1 1 1 0], Ypred=[0. 0. 0. 0.], Yerr=[1. 1. 1. 0.], dw=[1. 1. 3.]\n",
            "#1: w=[1. 1. 3.], Y=[1 1 1 0], Ypred=[1. 1. 1. 1.], Yerr=[ 0.  0.  0. -1.], dw=[-1. -1. -1.]\n",
            "#2: w=[0. 0. 2.], Y=[1 1 1 0], Ypred=[1. 1. 1. 1.], Yerr=[ 0.  0.  0. -1.], dw=[-1. -1. -1.]\n",
            "#3: w=[-1. -1.  1.], Y=[1 1 1 0], Ypred=[1. 0. 0. 0.], Yerr=[0. 1. 1. 0.], dw=[1. 1. 2.]\n",
            "#4: w=[0. 0. 3.], Y=[1 1 1 0], Ypred=[1. 1. 1. 1.], Yerr=[ 0.  0.  0. -1.], dw=[-1. -1. -1.]\n",
            "#5: w=[-1. -1.  2.], Y=[1 1 1 0], Ypred=[1. 1. 1. 0.], Yerr=[0. 0. 0. 0.], dw=[0. 0. 0.]\n",
            "#6: w=[-1. -1.  2.], Y=[1 1 1 0], Ypred=[1. 1. 1. 0.], Yerr=[0. 0. 0. 0.], dw=[0. 0. 0.]\n",
            "#7: w=[-1. -1.  2.], Y=[1 1 1 0], Ypred=[1. 1. 1. 0.], Yerr=[0. 0. 0. 0.], dw=[0. 0. 0.]\n",
            "#8: w=[-1. -1.  2.], Y=[1 1 1 0], Ypred=[1. 1. 1. 0.], Yerr=[0. 0. 0. 0.], dw=[0. 0. 0.]\n",
            "#9: w=[-1. -1.  2.], Y=[1 1 1 0], Ypred=[1. 1. 1. 0.], Yerr=[0. 0. 0. 0.], dw=[0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWvR709WnUlH",
        "colab_type": "text"
      },
      "source": [
        "We can confirm the learned parameter and classification results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_x4p1BldtQ-K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9216bda5-4cff-40f3-cdd0-5e25ff2796f6"
      },
      "source": [
        "w"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1., -1.,  2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E-P2RpWrtVyf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87f449b3-3c5d-4368-d540-c6826e5cbf00"
      },
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nkxBcCSTtDvm"
      },
      "source": [
        "### Stochastic gradient descent (SGD) with mini-batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bltpfNRctjV5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0636dc9c-92bd-4d4a-fb9e-2d274f45fd5e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(v):\n",
        "    return 1.0 / (1 + np.exp(-v))\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = sigmoid(np.dot(x, w))\n",
        "    print(f'#{t}: w={w}, Y={y}, Ypred={y_pred}, Yerr={y-y_pred}, dw={np.dot((y - y_pred), x)}')\n",
        "    w -= np.dot((y_pred - y), x)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0: w=[0. 0. 0.], Y=[1 1 1 0], Ypred=[0.5 0.5 0.5 0.5], Yerr=[ 0.5  0.5  0.5 -0.5], dw=[0. 0. 1.]\n",
            "#1: w=[0. 0. 1.], Y=[1 1 1 0], Ypred=[0.73105858 0.73105858 0.73105858 0.73105858], Yerr=[ 0.26894142  0.26894142  0.26894142 -0.73105858], dw=[-0.46211716 -0.46211716  0.07576569]\n",
            "#2: w=[-0.46211716 -0.46211716  1.07576569], Y=[1 1 1 0], Ypred=[0.74569184 0.64877263 0.64877263 0.53781052], Yerr=[ 0.25430816  0.35122737  0.35122737 -0.53781052], dw=[-0.18658315 -0.18658315  0.41895238]\n",
            "#3: w=[-0.64870031 -0.64870031  1.49471806], Y=[1 1 1 0], Ypred=[0.81678537 0.69973111 0.69973111 0.54916993], Yerr=[ 0.18321463  0.30026889  0.30026889 -0.54916993], dw=[-0.24890104 -0.24890104  0.23458248]\n",
            "#4: w=[-0.89760135 -0.89760135  1.72930054], Y=[1 1 1 0], Ypred=[0.84932293 0.6967141  0.6967141  0.48353042], Yerr=[ 0.15067707  0.3032859   0.3032859  -0.48353042], dw=[-0.18024452 -0.18024452  0.27371846]\n",
            "#5: w=[-1.07784586 -1.07784586  2.003019  ], Y=[1 1 1 0], Ypred=[0.88111369 0.71609499 0.71609499 0.46190578], Yerr=[ 0.11888631  0.28390501  0.28390501 -0.46190578], dw=[-0.17800078 -0.17800078  0.22479054]\n",
            "#6: w=[-1.25584664 -1.25584664  2.22780954], Y=[1 1 1 0], Ypred=[0.90271917 0.72551057 0.72551057 0.42950188], Yerr=[ 0.09728083  0.27448943  0.27448943 -0.42950188], dw=[-0.15501245 -0.15501245  0.21675781]\n",
            "#7: w=[-1.4108591  -1.4108591   2.44456734], Y=[1 1 1 0], Ypred=[0.92016326 0.73763419 0.73763419 0.40681426], Yerr=[ 0.07983674  0.26236581  0.26236581 -0.40681426], dw=[-0.14444845 -0.14444845  0.19775411]\n",
            "#8: w=[-1.55530754 -1.55530754  2.64232145], Y=[1 1 1 0], Ypred=[0.93353615 0.747819   0.747819   0.3850202 ], Yerr=[ 0.06646385  0.252181    0.252181   -0.3850202 ], dw=[-0.1328392  -0.1328392   0.18580565]\n",
            "#9: w=[-1.68814674 -1.68814674  2.8281271 ], Y=[1 1 1 0], Ypred=[0.94417697 0.75767603 0.75767603 0.36628993], Yerr=[ 0.05582303  0.24232397  0.24232397 -0.36628993], dw=[-0.12396596 -0.12396596  0.17418104]\n",
            "#10: w=[-1.8121127  -1.8121127   3.00230814], Y=[1 1 1 0], Ypred=[0.95267829 0.76677602 0.76677602 0.34934553], Yerr=[ 0.04732171  0.23322398  0.23322398 -0.34934553], dw=[-0.11612154 -0.11612154  0.16442415]\n",
            "#11: w=[-1.92823424 -1.92823424  3.16673229], Y=[1 1 1 0], Ypred=[0.95956298 0.77530247 0.77530247 0.33409176], Yerr=[ 0.04043702  0.22469753  0.22469753 -0.33409176], dw=[-0.10939423 -0.10939423  0.15574032]\n",
            "#12: w=[-2.03762847 -2.03762847  3.32247261], Y=[1 1 1 0], Ypred=[0.96519176 0.78327323 0.78327323 0.32021491], Yerr=[ 0.03480824  0.21672677  0.21672677 -0.32021491], dw=[-0.10348814 -0.10348814  0.14804688]\n",
            "#13: w=[-2.14111661 -2.14111661  3.47051948], Y=[1 1 1 0], Ypred=[0.96983722 0.79074185 0.79074185 0.30752543], Yerr=[ 0.03016278  0.20925815  0.20925815 -0.30752543], dw=[-0.09826728 -0.09826728  0.14115366]\n",
            "#14: w=[-2.23938389 -2.23938389  3.61167315], Y=[1 1 1 0], Ypred=[0.97370356 0.79774977 0.79774977 0.29585921], Yerr=[ 0.02629644  0.20225023  0.20225023 -0.29585921], dw=[-0.09360898 -0.09360898  0.1349377 ]\n",
            "#15: w=[-2.33299286 -2.33299286  3.74661085], Y=[1 1 1 0], Ypred=[0.97694642 0.80433597 0.80433597 0.28508529], Yerr=[ 0.02305358  0.19566403  0.19566403 -0.28508529], dw=[-0.08942125 -0.08942125  0.12929636]\n",
            "#16: w=[-2.42241411 -2.42241411  3.8759072 ], Y=[1 1 1 0], Ypred=[0.97968571 0.81053544 0.81053544 0.27509562], Yerr=[ 0.02031429  0.18946456  0.18946456 -0.27509562], dw=[-0.08563106 -0.08563106  0.12414779]\n",
            "#17: w=[-2.50804517 -2.50804517  4.00005499], Y=[1 1 1 0], Ypred=[0.98201476 0.81637974 0.81637974 0.26580038], Yerr=[ 0.01798524  0.18362026  0.18362026 -0.26580038], dw=[-0.08218013 -0.08218013  0.11942537]\n",
            "#18: w=[-2.5902253  -2.5902253   4.11948036], Y=[1 1 1 0], Ypred=[0.98400698 0.82189729 0.82189729 0.25712408], Yerr=[ 0.01599302  0.17810271  0.17810271 -0.25712408], dw=[-0.07902138 -0.07902138  0.11507435]\n",
            "#19: w=[-2.66924668 -2.66924668  4.23455471], Y=[1 1 1 0], Ypred=[0.9857206  0.8271137  0.8271137  0.24900264], Yerr=[ 0.0142794   0.1728863   0.1728863  -0.24900264], dw=[-0.07611634 -0.07611634  0.11104936]\n",
            "#20: w=[-2.74536302 -2.74536302  4.34560407], Y=[1 1 1 0], Ypred=[0.98720223 0.83205207 0.83205207 0.2413812 ], Yerr=[ 0.01279777  0.16794793  0.16794793 -0.2413812 ], dw=[-0.07343328 -0.07343328  0.10731242]\n",
            "#21: w=[-2.8187963  -2.8187963   4.45291649], Y=[1 1 1 0], Ypred=[0.98848948 0.83673328 0.83673328 0.23421246], Yerr=[ 0.01151052  0.16326672  0.16326672 -0.23421246], dw=[-0.07094575 -0.07094575  0.10383149]\n",
            "#22: w=[-2.88974204 -2.88974204  4.55674799], Y=[1 1 1 0], Ypred=[0.98961289 0.84117623 0.84117623 0.22745531], Yerr=[ 0.01038711  0.15882377  0.15882377 -0.22745531], dw=[-0.06863153 -0.06863153  0.10057935]\n",
            "#23: w=[-2.95837358 -2.95837358  4.65732734], Y=[1 1 1 0], Ypred=[0.99059745 0.84539804 0.84539804 0.22107378], Yerr=[ 0.00940255  0.15460196  0.15460196 -0.22107378], dw=[-0.06647182 -0.06647182  0.09753268]\n",
            "#24: w=[-3.0248454  -3.0248454   4.75486002], Y=[1 1 1 0], Ypred=[0.99146375 0.84941429 0.84941429 0.21503627], Yerr=[ 0.00853625  0.15058571  0.15058571 -0.21503627], dw=[-0.06445056 -0.06445056  0.0946714 ]\n",
            "#25: w=[-3.08929596 -3.08929596  4.84953142], Y=[1 1 1 0], Ypred=[0.99222882 0.85323915 0.85323915 0.20931481], Yerr=[ 0.00777118  0.14676085  0.14676085 -0.20931481], dw=[-0.06255396 -0.06255396  0.09197808]\n",
            "#26: w=[-3.15184992 -3.15184992  4.9415095 ], Y=[1 1 1 0], Ypred=[0.99290687 0.85688553 0.85688553 0.20388454], Yerr=[ 0.00709313  0.14311447  0.14311447 -0.20388454], dw=[-0.06077008 -0.06077008  0.08943752]\n",
            "#27: w=[-3.21262    -3.21262     5.03094702], Y=[1 1 1 0], Ypred=[0.99350978 0.86036526 0.86036526 0.19872329], Yerr=[ 0.00649022  0.13963474  0.13963474 -0.19872329], dw=[-0.05908855 -0.05908855  0.08703641]\n",
            "#28: w=[-3.27170855 -3.27170855  5.11798343], Y=[1 1 1 0], Ypred=[0.99404756 0.86368914 0.86368914 0.19381117], Yerr=[ 0.00595244  0.13631086  0.13631086 -0.19381117], dw=[-0.0575003 -0.0575003  0.084763 ]\n",
            "#29: w=[-3.32920886 -3.32920886  5.20274643], Y=[1 1 1 0], Ypred=[0.99452867 0.86686707 0.86686707 0.18913028], Yerr=[ 0.00547133  0.13313293  0.13313293 -0.18913028], dw=[-0.05599736 -0.05599736  0.0826069 ]\n",
            "#30: w=[-3.38520622 -3.38520622  5.28535333], Y=[1 1 1 0], Ypred=[0.99496029 0.86990818 0.86990818 0.18466449], Yerr=[ 0.00503971  0.13009182  0.13009182 -0.18466449], dw=[-0.05457266 -0.05457266  0.08055888]\n",
            "#31: w=[-3.43977888 -3.43977888  5.36591221], Y=[1 1 1 0], Ypred=[0.99534854 0.87282082 0.87282082 0.18039915], Yerr=[ 0.00465146  0.12717918  0.12717918 -0.18039915], dw=[-0.05321997 -0.05321997  0.07861067]\n",
            "#32: w=[-3.49299885 -3.49299885  5.44452288], Y=[1 1 1 0], Ypred=[0.99569868 0.87561273 0.87561273 0.17632098], Yerr=[ 0.00430132  0.12438727  0.12438727 -0.17632098], dw=[-0.05193371 -0.05193371  0.07675489]\n",
            "#33: w=[-3.54493256 -3.54493256  5.52127776], Y=[1 1 1 0], Ypred=[0.99601521 0.87829102 0.87829102 0.17241787], Yerr=[ 0.00398479  0.12170898  0.12170898 -0.17241787], dw=[-0.05070889 -0.05070889  0.07498488]\n",
            "#34: w=[-3.59564145 -3.59564145  5.59626265], Y=[1 1 1 0], Ypred=[0.99630202 0.88086228 0.88086228 0.16867876], Yerr=[ 0.00369798  0.11913772  0.11913772 -0.16867876], dw=[-0.04954104 -0.04954104  0.07329466]\n",
            "#35: w=[-3.64518249 -3.64518249  5.66955731], Y=[1 1 1 0], Ypred=[0.99656247 0.88333262 0.88333262 0.16509351], Yerr=[ 0.00343753  0.11666738  0.11666738 -0.16509351], dw=[-0.04842613 -0.04842613  0.07167879]\n",
            "#36: w=[-3.69360862 -3.69360862  5.74123609], Y=[1 1 1 0], Ypred=[0.99679948 0.88570767 0.88570767 0.16165285], Yerr=[ 0.00320052  0.11429233  0.11429233 -0.16165285], dw=[-0.04736052 -0.04736052  0.07013233]\n",
            "#37: w=[-3.74096913 -3.74096913  5.81136842], Y=[1 1 1 0], Ypred=[0.9970156  0.88799268 0.88799268 0.15834822], Yerr=[ 0.0029844   0.11200732  0.11200732 -0.15834822], dw=[-0.0463409  -0.0463409   0.06865082]\n",
            "#38: w=[-3.78731004 -3.78731004  5.88001924], Y=[1 1 1 0], Ypred=[0.99721306 0.89019253 0.89019253 0.15517174], Yerr=[ 0.00278694  0.10980747  0.10980747 -0.15517174], dw=[-0.04536427 -0.04536427  0.06723014]\n",
            "#39: w=[-3.83267431 -3.83267431  5.94724938], Y=[1 1 1 0], Ypred=[0.99739379 0.89231175 0.89231175 0.15211616], Yerr=[ 0.00260621  0.10768825  0.10768825 -0.15211616], dw=[-0.0444279  -0.0444279   0.06586656]\n",
            "#40: w=[-3.87710221 -3.87710221  6.01311594], Y=[1 1 1 0], Ypred=[0.99755952 0.89435456 0.89435456 0.14917473], Yerr=[ 0.00244048  0.10564544  0.10564544 -0.14917473], dw=[-0.04352929 -0.04352929  0.06455663]\n",
            "#41: w=[-3.9206315  -3.9206315   6.07767257], Y=[1 1 1 0], Ypred=[0.99771174 0.89632491 0.89632491 0.14634123], Yerr=[ 0.00228826  0.10367509  0.10367509 -0.14634123], dw=[-0.04266614 -0.04266614  0.06329721]\n",
            "#42: w=[-3.96329764 -3.96329764  6.14096978], Y=[1 1 1 0], Ypred=[0.99785179 0.89822647 0.89822647 0.14360989], Yerr=[ 0.00214821  0.10177353  0.10177353 -0.14360989], dw=[-0.04183635 -0.04183635  0.06208539]\n",
            "#43: w=[-4.005134   -4.005134    6.20305517], Y=[1 1 1 0], Ypred=[0.99798085 0.90006268 0.90006268 0.14097532], Yerr=[ 0.00201915  0.09993732  0.09993732 -0.14097532], dw=[-0.041038   -0.041038    0.06091848]\n",
            "#44: w=[-4.04617199 -4.04617199  6.26397365], Y=[1 1 1 0], Ypred=[0.99809995 0.90183675 0.90183675 0.13843253], Yerr=[ 0.00190005  0.09816325  0.09816325 -0.13843253], dw=[-0.04026928 -0.04026928  0.05979401]\n",
            "#45: w=[-4.08644127 -4.08644127  6.32376767], Y=[1 1 1 0], Ypred=[0.99821004 0.90355172 0.90355172 0.13597685], Yerr=[ 0.00178996  0.09644828  0.09644828 -0.13597685], dw=[-0.03952857 -0.03952857  0.05870968]\n",
            "#46: w=[-4.12596984 -4.12596984  6.38247735], Y=[1 1 1 0], Ypred=[0.99831193 0.90521038 0.90521038 0.13360395], Yerr=[ 0.00168807  0.09478962  0.09478962 -0.13360395], dw=[-0.03881433 -0.03881433  0.05766336]\n",
            "#47: w=[-4.16478417 -4.16478417  6.44014071], Y=[1 1 1 0], Ypred=[0.99840636 0.90681541 0.90681541 0.13130974], Yerr=[ 0.00159364  0.09318459  0.09318459 -0.13130974], dw=[-0.03812515 -0.03812515  0.05665308]\n",
            "#48: w=[-4.20290933 -4.20290933  6.49679379], Y=[1 1 1 0], Ypred=[0.998494   0.90836928 0.90836928 0.12909044], Yerr=[ 0.001506    0.09163072  0.09163072 -0.12909044], dw=[-0.03745973 -0.03745973  0.05567698]\n",
            "#49: w=[-4.24036905 -4.24036905  6.55247077], Y=[1 1 1 0], Ypred=[0.99857545 0.90987435 0.90987435 0.12694248], Yerr=[ 0.00142455  0.09012565  0.09012565 -0.12694248], dw=[-0.03681683 -0.03681683  0.05473337]\n",
            "#50: w=[-4.27718589 -4.27718589  6.60720414], Y=[1 1 1 0], Ypred=[0.99865122 0.91133281 0.91133281 0.12486253], Yerr=[ 0.00134878  0.08866719  0.08866719 -0.12486253], dw=[-0.03619534 -0.03619534  0.05382063]\n",
            "#51: w=[-4.31338123 -4.31338123  6.66102477], Y=[1 1 1 0], Ypred=[0.9987218  0.91274674 0.91274674 0.12284744], Yerr=[ 0.0012782   0.08725326  0.08725326 -0.12284744], dw=[-0.03559418 -0.03559418  0.05293728]\n",
            "#52: w=[-4.34897541 -4.34897541  6.71396205], Y=[1 1 1 0], Ypred=[0.99878763 0.9141181  0.9141181  0.12089427], Yerr=[ 0.00121237  0.0858819   0.0858819  -0.12089427], dw=[-0.03501237 -0.03501237  0.05208191]\n",
            "#53: w=[-4.38398778 -4.38398778  6.76604395], Y=[1 1 1 0], Ypred=[0.99884908 0.91544872 0.91544872 0.11900026], Yerr=[ 0.00115092  0.08455128  0.08455128 -0.11900026], dw=[-0.03444899 -0.03444899  0.05125321]\n",
            "#54: w=[-4.41843677 -4.41843677  6.81729716], Y=[1 1 1 0], Ypred=[0.99890652 0.91674036 0.91674036 0.1171628 ], Yerr=[ 0.00109348  0.08325964  0.08325964 -0.1171628 ], dw=[-0.03390316 -0.03390316  0.05044995]\n",
            "#55: w=[-4.45233993 -4.45233993  6.86774711], Y=[1 1 1 0], Ypred=[0.99896026 0.91799466 0.91799466 0.11537943], Yerr=[ 0.00103974  0.08200534  0.08200534 -0.11537943], dw=[-0.03337409 -0.03337409  0.04967099]\n",
            "#56: w=[-4.48571402 -4.48571402  6.9174181 ], Y=[1 1 1 0], Ypred=[0.9990106  0.91921317 0.91921317 0.11364783], Yerr=[ 0.0009894   0.08078683  0.08078683 -0.11364783], dw=[-0.032861   -0.032861    0.04891524]\n",
            "#57: w=[-4.51857501 -4.51857501  6.96633334], Y=[1 1 1 0], Ypred=[0.99905778 0.92039737 0.92039737 0.11196581], Yerr=[ 0.00094222  0.07960263  0.07960263 -0.11196581], dw=[-0.03236318 -0.03236318  0.04818167]\n",
            "#58: w=[-4.55093819 -4.55093819  7.01451501], Y=[1 1 1 0], Ypred=[0.99910207 0.92154865 0.92154865 0.11033131], Yerr=[ 0.00089793  0.07845135  0.07845135 -0.11033131], dw=[-0.03187996 -0.03187996  0.04746933]\n",
            "#59: w=[-4.58281815 -4.58281815  7.06198434], Y=[1 1 1 0], Ypred=[0.99914366 0.92266833 0.92266833 0.10874238], Yerr=[ 0.00085634  0.07733167  0.07733167 -0.10874238], dw=[-0.0314107  -0.0314107   0.04677731]\n",
            "#60: w=[-4.61422885 -4.61422885  7.10876166], Y=[1 1 1 0], Ypred=[0.99918276 0.92375766 0.92375766 0.10719716], Yerr=[ 0.00081724  0.07624234  0.07624234 -0.10719716], dw=[-0.03095482 -0.03095482  0.04610476]\n",
            "#61: w=[-4.64518367 -4.64518367  7.15486642], Y=[1 1 1 0], Ypred=[0.99921956 0.92481784 0.92481784 0.1056939 ], Yerr=[ 0.00078044  0.07518216  0.07518216 -0.1056939 ], dw=[-0.03051174 -0.03051174  0.04545087]\n",
            "#62: w=[-4.6756954  -4.6756954   7.20031729], Y=[1 1 1 0], Ypred=[0.99925421 0.92584998 0.92584998 0.10423095], Yerr=[ 0.00074579  0.07415002  0.07415002 -0.10423095], dw=[-0.03008093 -0.03008093  0.04481488]\n",
            "#63: w=[-4.70577633 -4.70577633  7.24513217], Y=[1 1 1 0], Ypred=[0.99928687 0.92685517 0.92685517 0.10280673], Yerr=[ 0.00071313  0.07314483  0.07314483 -0.10280673], dw=[-0.0296619  -0.0296619   0.04419607]\n",
            "#64: w=[-4.73543823 -4.73543823  7.28932824], Y=[1 1 1 0], Ypred=[0.99931768 0.92783441 0.92783441 0.10141975], Yerr=[ 0.00068232  0.07216559  0.07216559 -0.10141975], dw=[-0.02925416 -0.02925416  0.04359375]\n",
            "#65: w=[-4.76469239 -4.76469239  7.33292199], Y=[1 1 1 0], Ypred=[0.99934677 0.92878869 0.92878869 0.10006858], Yerr=[ 0.00065323  0.07121131  0.07121131 -0.10006858], dw=[-0.02885727 -0.02885727  0.04300727]\n",
            "#66: w=[-4.79354966 -4.79354966  7.37592926], Y=[1 1 1 0], Ypred=[0.99937425 0.92971891 0.92971891 0.09875189], Yerr=[ 0.00062575  0.07028109  0.07028109 -0.09875189], dw=[-0.0284708  -0.0284708   0.04243603]\n",
            "#67: w=[-4.82202047 -4.82202047  7.41836529], Y=[1 1 1 0], Ypred=[0.99940023 0.93062597 0.93062597 0.09746839], Yerr=[ 0.00059977  0.06937403  0.06937403 -0.09746839], dw=[-0.02809435 -0.02809435  0.04187945]\n",
            "#68: w=[-4.85011482 -4.85011482  7.46024474], Y=[1 1 1 0], Ypred=[0.99942482 0.93151069 0.93151069 0.09621685], Yerr=[ 0.00057518  0.06848931  0.06848931 -0.09621685], dw=[-0.02772754 -0.02772754  0.04133696]\n",
            "#69: w=[-4.87784236 -4.87784236  7.5015817 ], Y=[1 1 1 0], Ypred=[0.99944809 0.93237386 0.93237386 0.09499613], Yerr=[ 0.00055191  0.06762614  0.06762614 -0.09499613], dw=[-0.02737    -0.02737     0.04080804]\n",
            "#70: w=[-4.90521236 -4.90521236  7.54238974], Y=[1 1 1 0], Ypred=[0.99947015 0.93321626 0.93321626 0.09380512], Yerr=[ 0.00052985  0.06678374  0.06678374 -0.09380512], dw=[-0.02702138 -0.02702138  0.0402922 ]\n",
            "#71: w=[-4.93223374 -4.93223374  7.58268195], Y=[1 1 1 0], Ypred=[0.99949107 0.93403861 0.93403861 0.09264275], Yerr=[ 0.00050893  0.06596139  0.06596139 -0.09264275], dw=[-0.02668136 -0.02668136  0.03978896]\n",
            "#72: w=[-4.9589151  -4.9589151   7.62247091], Y=[1 1 1 0], Ypred=[0.99951091 0.9348416  0.9348416  0.09150803], Yerr=[ 0.00048909  0.0651584   0.0651584  -0.09150803], dw=[-0.02634963 -0.02634963  0.03929787]\n",
            "#73: w=[-4.98526473 -4.98526473  7.66176878], Y=[1 1 1 0], Ypred=[0.99952975 0.93562588 0.93562588 0.0904    ], Yerr=[ 0.00047025  0.06437412  0.06437412 -0.0904    ], dw=[-0.02602588 -0.02602588  0.03881849]\n",
            "#74: w=[-5.01129061 -5.01129061  7.70058727], Y=[1 1 1 0], Ypred=[0.99954764 0.9363921  0.9363921  0.08931774], Yerr=[ 0.00045236  0.0636079   0.0636079  -0.08931774], dw=[-0.02570984 -0.02570984  0.03835041]\n",
            "#75: w=[-5.03700045 -5.03700045  7.73893768], Y=[1 1 1 0], Ypred=[0.99956466 0.93714086 0.93714086 0.08826037], Yerr=[ 0.00043534  0.06285914  0.06285914 -0.08826037], dw=[-0.02540123 -0.02540123  0.03789325]\n",
            "#76: w=[-5.06240168 -5.06240168  7.77683093], Y=[1 1 1 0], Ypred=[0.99958084 0.93787273 0.93787273 0.08722707], Yerr=[ 0.00041916  0.06212727  0.06212727 -0.08722707], dw=[-0.0250998  -0.0250998   0.03744663]\n",
            "#77: w=[-5.08750148 -5.08750148  7.81427757], Y=[1 1 1 0], Ypred=[0.99959624 0.93858827 0.93858827 0.08621703], Yerr=[ 0.00040376  0.06141173  0.06141173 -0.08621703], dw=[-0.0248053  -0.0248053   0.03701019]\n",
            "#78: w=[-5.11230678 -5.11230678  7.85128776], Y=[1 1 1 0], Ypred=[0.9996109  0.93928801 0.93928801 0.08522948], Yerr=[ 0.0003891   0.06071199  0.06071199 -0.08522948], dw=[-0.0245175  -0.0245175   0.03658359]\n",
            "#79: w=[-5.13682428 -5.13682428  7.88787135], Y=[1 1 1 0], Ypred=[0.99962487 0.93997246 0.93997246 0.08426371], Yerr=[ 0.00037513  0.06002754  0.06002754 -0.08426371], dw=[-0.02423616 -0.02423616  0.03616651]\n",
            "#80: w=[-5.16106044 -5.16106044  7.92403786], Y=[1 1 1 0], Ypred=[0.99963819 0.94064209 0.94064209 0.08331899], Yerr=[ 0.00036181  0.05935791  0.05935791 -0.08331899], dw=[-0.02396109 -0.02396109  0.03575862]\n",
            "#81: w=[-5.18502153 -5.18502153  7.95979648], Y=[1 1 1 0], Ypred=[0.9996509  0.94129739 0.94129739 0.08239467], Yerr=[ 0.0003491   0.05870261  0.05870261 -0.08239467], dw=[-0.02369206 -0.02369206  0.03535965]\n",
            "#82: w=[-5.20871359 -5.20871359  7.99515613], Y=[1 1 1 0], Ypred=[0.99966302 0.94193879 0.94193879 0.08149011], Yerr=[ 0.00033698  0.05806121  0.05806121 -0.08149011], dw=[-0.0234289  -0.0234289   0.03496929]\n",
            "#83: w=[-5.23214249 -5.23214249  8.03012542], Y=[1 1 1 0], Ypred=[0.9996746  0.94256673 0.94256673 0.08060467], Yerr=[ 0.0003254   0.05743327  0.05743327 -0.08060467], dw=[-0.0231714  -0.0231714   0.03458727]\n",
            "#84: w=[-5.25531389 -5.25531389  8.06471269], Y=[1 1 1 0], Ypred=[0.99968566 0.94318161 0.94318161 0.07973778], Yerr=[ 0.00031434  0.05681839  0.05681839 -0.07973778], dw=[-0.02291939 -0.02291939  0.03421334]\n",
            "#85: w=[-5.27823329 -5.27823329  8.09892603], Y=[1 1 1 0], Ypred=[0.99969623 0.94378383 0.94378383 0.07888887], Yerr=[ 0.00030377  0.05621617  0.05621617 -0.07888887], dw=[-0.0226727  -0.0226727   0.03384724]\n",
            "#86: w=[-5.30090599 -5.30090599  8.13277327], Y=[1 1 1 0], Ypred=[0.99970633 0.94437378 0.94437378 0.07805739], Yerr=[ 0.00029367  0.05562622  0.05562622 -0.07805739], dw=[-0.02243116 -0.02243116  0.03348873]\n",
            "#87: w=[-5.32333715 -5.32333715  8.166262  ], Y=[1 1 1 0], Ypred=[0.999716   0.9449518  0.9449518  0.07724281], Yerr=[ 0.000284    0.0550482   0.0550482  -0.07724281], dw=[-0.02219461 -0.02219461  0.03313758]\n",
            "#88: w=[-5.34553176 -5.34553176  8.19939957], Y=[1 1 1 0], Ypred=[0.99972526 0.94551827 0.94551827 0.07644464], Yerr=[ 0.00027474  0.05448173  0.05448173 -0.07644464], dw=[-0.02196291 -0.02196291  0.03279356]\n",
            "#89: w=[-5.36749467 -5.36749467  8.23219314], Y=[1 1 1 0], Ypred=[0.99973412 0.94607351 0.94607351 0.07566239], Yerr=[ 0.00026588  0.05392649  0.05392649 -0.07566239], dw=[-0.0217359  -0.0217359   0.03245647]\n",
            "#90: w=[-5.38923057 -5.38923057  8.26464961], Y=[1 1 1 0], Ypred=[0.99974261 0.94661785 0.94661785 0.07489559], Yerr=[ 0.00025739  0.05338215  0.05338215 -0.07489559], dw=[-0.02151344 -0.02151344  0.0321261 ]\n",
            "#91: w=[-5.41074401 -5.41074401  8.29677571], Y=[1 1 1 0], Ypred=[0.99975074 0.9471516  0.9471516  0.07414381], Yerr=[ 0.00024926  0.0528484   0.0528484  -0.07414381], dw=[-0.02129541 -0.02129541  0.03180225]\n",
            "#92: w=[-5.43203942 -5.43203942  8.32857796], Y=[1 1 1 0], Ypred=[0.99975854 0.94767506 0.94767506 0.07340661], Yerr=[ 0.00024146  0.05232494  0.05232494 -0.07340661], dw=[-0.02108167 -0.02108167  0.03148473]\n",
            "#93: w=[-5.45312109 -5.45312109  8.36006269], Y=[1 1 1 0], Ypred=[0.99976603 0.94818852 0.94818852 0.07268357], Yerr=[ 0.00023397  0.05181148  0.05181148 -0.07268357], dw=[-0.02087209 -0.02087209  0.03117336]\n",
            "#94: w=[-5.47399318 -5.47399318  8.39123606], Y=[1 1 1 0], Ypred=[0.9997732  0.94869226 0.94869226 0.0719743 ], Yerr=[ 0.0002268   0.05130774  0.05130774 -0.0719743 ], dw=[-0.02066656 -0.02066656  0.03086797]\n",
            "#95: w=[-5.49465974 -5.49465974  8.42210403], Y=[1 1 1 0], Ypred=[0.9997801  0.94918655 0.94918655 0.07127841], Yerr=[ 0.0002199   0.05081345  0.05081345 -0.07127841], dw=[-0.02046497 -0.02046497  0.03056839]\n",
            "#96: w=[-5.51512471 -5.51512471  8.45267241], Y=[1 1 1 0], Ypred=[0.99978672 0.94967165 0.94967165 0.07059554], Yerr=[ 0.00021328  0.05032835  0.05032835 -0.07059554], dw=[-0.02026719 -0.02026719  0.03027445]\n",
            "#97: w=[-5.5353919  -5.5353919   8.48294686], Y=[1 1 1 0], Ypred=[0.99979307 0.9501478  0.9501478  0.06992533], Yerr=[ 0.00020693  0.0498522   0.0498522  -0.06992533], dw=[-0.02007313 -0.02007313  0.02998599]\n",
            "#98: w=[-5.55546503 -5.55546503  8.51293285], Y=[1 1 1 0], Ypred=[0.99979919 0.95061525 0.95061525 0.06926743], Yerr=[ 0.00020081  0.04938475  0.04938475 -0.06926743], dw=[-0.01988268 -0.01988268  0.02970288]\n",
            "#99: w=[-5.57534771 -5.57534771  8.54263573], Y=[1 1 1 0], Ypred=[0.99980506 0.95107424 0.95107424 0.06862151], Yerr=[ 0.00019494  0.04892576  0.04892576 -0.06862151], dw=[-0.01969575 -0.01969575  0.02942495]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9bASDMfhtm-I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "722d9b97-51a6-4dea-f6f9-16bcce99df9a"
      },
      "source": [
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.59504346, -5.59504346,  8.57206068])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-69r0c4KtqlW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0febbab-bba1-45d5-8f5f-a5de8fc54bee"
      },
      "source": [
        "sigmoid(np.dot(x, w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.99981071, 0.95152498, 0.95152498, 0.06798725])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ELUeNFRFuJv3"
      },
      "source": [
        "## Automatic differentiation\n",
        "\n",
        "Consider a loss function,\n",
        "$$\n",
        "l_{\\boldsymbol{x}}(\\boldsymbol{w}) = - \\log \\sigma(\\boldsymbol{w} \\cdot \\boldsymbol{x}) = - \\log \\frac{1}{1 + e^{-\\boldsymbol{w} \\cdot \\boldsymbol{x}}}\n",
        "$$\n",
        "\n",
        "This section shows implementations in different libraries of deep learning for computing the loss value $l_{\\boldsymbol{x}}(\\boldsymbol{w})$ and gradients $\\frac{\\partial l_{\\boldsymbol{x}}(\\boldsymbol{w})}{\\partial \\boldsymbol{w}}$ when $\\boldsymbol{x} = (1, 1, 1)$ and $\\boldsymbol{w} = (1, 1, -1.5)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aB0DwVOyuXP_"
      },
      "source": [
        "### Using autograd\n",
        "\n",
        "See: https://github.com/HIPS/autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DyC9iOFO-1cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9a209514-7ade-4e85-c35d-6a57ab70f91b"
      },
      "source": [
        "import autograd\n",
        "import autograd.numpy as np\n",
        "\n",
        "def loss(w, x):\n",
        "    return -np.log(1.0 / (1 + np.exp(-np.dot(x, w))))\n",
        "\n",
        "x = np.array([1, 1, 1])\n",
        "w = np.array([1.0, 1.0, -1.5])\n",
        "\n",
        "grad_loss = autograd.grad(loss)\n",
        "print(loss(w, x))\n",
        "print(grad_loss(w, x))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47407698418010663\n",
            "[-0.37754067 -0.37754067 -0.37754067]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "59D2aARTuQDL"
      },
      "source": [
        "### Using pytorch\n",
        "\n",
        "See: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpZWYFRQuvo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "x = torch.tensor([1, 1, 1], dtype=dtype)\n",
        "w = torch.tensor([1.0, 1.0, -1.5], dtype=dtype, requires_grad=True)\n",
        "\n",
        "loss = -torch.dot(x, w).sigmoid().log()\n",
        "loss.backward()\n",
        "print(loss.item())\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ_BH643pO9Y",
        "colab_type": "text"
      },
      "source": [
        "### Using TensorFlow Eager\n",
        "\n",
        "See: https://www.tensorflow.org/guide/autodiff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp87EyeAp6CY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "db001069-9316-473b-c1d8-1fab2ce22e88"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "dtype = tf.float32\n",
        "\n",
        "x = tf.constant([1, 1, 1], dtype=dtype, name='x')\n",
        "w = tf.Variable([1.0, 1.0, -1.5], dtype=dtype, name='w')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    loss = -tf.math.log(tf.math.sigmoid(tf.tensordot(x, w, 1)))\n",
        "\n",
        "print(loss.numpy())\n",
        "print(tape.gradient(loss, w))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47407696\n",
            "tf.Tensor([-0.37754062 -0.37754062 -0.37754062], shape=(3,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM4eI6J0pXJF",
        "colab_type": "text"
      },
      "source": [
        "### Using JAX\n",
        "\n",
        "See: https://github.com/google/jax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI-UjxJzsnqY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "162c5a37-eaf4-4e46-bc2e-731450be8cb3"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as np\n",
        "\n",
        "def loss(w, x):\n",
        "    return -np.log(1.0 / (1 + np.exp(-np.dot(x, w))))\n",
        "\n",
        "x = np.array([1, 1, 1])\n",
        "w = np.array([1.0, 1.0, -1.5])\n",
        "\n",
        "grad_loss = jax.jit(jax.grad(loss))\n",
        "print(loss(w, x))\n",
        "print(grad_loss(w, x))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47407696\n",
            "[-0.37754068 -0.37754068 -0.37754068]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VFzuau5gu4vY"
      },
      "source": [
        "## Implementing neural networks with pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "STwWdvJCva4G"
      },
      "source": [
        "### Single-layer neural network using automatic differentiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xOJHDGYKIgsm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c3e2574-463f-427d-c13a-5ecceb35f30d"
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "w = torch.randn(3, 1, dtype=dtype, requires_grad=True)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    # y_pred = \\sigma(x \\cdot w)\n",
        "    y_pred = x.mm(w).sigmoid()\n",
        "    ll = y * y_pred + (1 - y) * (1 - y_pred)\n",
        "    loss = -ll.log().sum()      # The loss value.\n",
        "    loss.backward()             # Compute the gradients of the loss.\n",
        "\n",
        "    print(f'#{t}: w={w.t().data[0].numpy()}, loss={loss.item()}')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w -= eta * w.grad       # Update weights using SGD.        \n",
        "        w.grad.zero_()          # Clear the gradients for the next iteration."
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0: w=[-2.0885355  -1.4806837  -0.15213448], loss=4.949068546295166\n",
            "#1: w=[-1.6484289  -1.0742207   0.97523004], loss=2.3098151683807373\n",
            "#2: w=[-1.3915058 -0.8860434  1.6314366], loss=1.5687907934188843\n",
            "#3: w=[-1.3432871 -0.8970635  1.922385 ], loss=1.43471097946167\n",
            "#4: w=[-1.3743029 -0.9756532  2.0871632], loss=1.370697021484375\n",
            "#5: w=[-1.4271579 -1.069197   2.2132783], loss=1.3172904253005981\n",
            "#6: w=[-1.4855065 -1.1632609  2.3249977], loss=1.2688961029052734\n",
            "#7: w=[-1.5445642 -1.2539637  2.4296527], loss=1.2244752645492554\n",
            "#8: w=[-1.6029084 -1.340456   2.5296922], loss=1.1834886074066162\n",
            "#9: w=[-1.6601355 -1.4227742  2.626064 ], loss=1.1455150842666626\n",
            "#10: w=[-1.7161564 -1.5011961  2.7192283], loss=1.1102027893066406\n",
            "#11: w=[-1.7709782 -1.5760455  2.809472 ], loss=1.0772548913955688\n",
            "#12: w=[-1.8246365 -1.6476331  2.8970122], loss=1.0464177131652832\n",
            "#13: w=[-1.877173  -1.7162416  2.9820302], loss=1.01747465133667\n",
            "#14: w=[-1.9286293 -1.7821238  3.0646856], loss=0.9902386665344238\n",
            "#15: w=[-1.9790448 -1.8455048  3.145121 ], loss=0.964547872543335\n",
            "#16: w=[-2.0284567 -1.9065844  3.223466 ], loss=0.9402616024017334\n",
            "#17: w=[-2.0769    -1.965541   3.2998374], loss=0.9172563552856445\n",
            "#18: w=[-2.1244075 -2.0225337  3.374343 ], loss=0.8954237699508667\n",
            "#19: w=[-2.1710105 -2.077705   3.4470809], loss=0.8746681213378906\n",
            "#20: w=[-2.2167385 -2.1311827  3.518141 ], loss=0.8549047708511353\n",
            "#21: w=[-2.2616198 -2.1830816  3.587607 ], loss=0.8360580801963806\n",
            "#22: w=[-2.3056812 -2.2335055  3.6555548], loss=0.8180602788925171\n",
            "#23: w=[-2.3489485 -2.2825482  3.7220557], loss=0.800850510597229\n",
            "#24: w=[-2.3914464 -2.3302946  3.7871752], loss=0.7843745946884155\n",
            "#25: w=[-2.4331987 -2.3768218  3.850974 ], loss=0.7685827016830444\n",
            "#26: w=[-2.474228  -2.4222002  3.9135091], loss=0.7534305453300476\n",
            "#27: w=[-2.514556  -2.466494   3.9748333], loss=0.7388774156570435\n",
            "#28: w=[-2.554204 -2.509762  4.034996], loss=0.7248860597610474\n",
            "#29: w=[-2.593192  -2.552058   4.0940433], loss=0.711422324180603\n",
            "#30: w=[-2.6315396 -2.5934312  4.1520185], loss=0.6984560489654541\n",
            "#31: w=[-2.669265  -2.6339276  4.2089624], loss=0.6859581470489502\n",
            "#32: w=[-2.7063866 -2.673589   4.2649126], loss=0.6739023923873901\n",
            "#33: w=[-2.7429216 -2.7124543  4.3199058], loss=0.6622648239135742\n",
            "#34: w=[-2.7788866 -2.7505596  4.3739753], loss=0.6510227918624878\n",
            "#35: w=[-2.8142977 -2.787938   4.4271536], loss=0.6401559114456177\n",
            "#36: w=[-2.8491702 -2.824621   4.4794707], loss=0.6296449303627014\n",
            "#37: w=[-2.8835192 -2.8606372  4.5309553], loss=0.6194717288017273\n",
            "#38: w=[-2.9173589 -2.896014   4.5816345], loss=0.6096200346946716\n",
            "#39: w=[-2.9507031 -2.9307764  4.6315336], loss=0.600074052810669\n",
            "#40: w=[-2.983565 -2.964948  4.680678], loss=0.5908195972442627\n",
            "#41: w=[-3.0159578 -2.998551   4.7290897], loss=0.5818429589271545\n",
            "#42: w=[-3.0478935 -3.0316062  4.776792 ], loss=0.5731315016746521\n",
            "#43: w=[-3.079384  -3.0641334  4.8238053], loss=0.5646734237670898\n",
            "#44: w=[-3.1104412 -3.0961509  4.87015  ], loss=0.5564577579498291\n",
            "#45: w=[-3.1410756 -3.127676   4.9158454], loss=0.548473596572876\n",
            "#46: w=[-3.1712983 -3.1587253  4.9609094], loss=0.5407111644744873\n",
            "#47: w=[-3.2011194 -3.1893141  5.0053596], loss=0.5331617593765259\n",
            "#48: w=[-3.2305493 -3.2194574  5.049213 ], loss=0.525816023349762\n",
            "#49: w=[-3.2595973 -3.2491689  5.0924854], loss=0.5186655521392822\n",
            "#50: w=[-3.2882726 -3.2784617  5.1351924], loss=0.5117030739784241\n",
            "#51: w=[-3.3165843 -3.3073487  5.1773486], loss=0.5049207806587219\n",
            "#52: w=[-3.3445413 -3.335842   5.2189684], loss=0.4983120560646057\n",
            "#53: w=[-3.3721516 -3.3639524  5.260065 ], loss=0.4918699860572815\n",
            "#54: w=[-3.3994236 -3.3916912  5.3006516], loss=0.4855882525444031\n",
            "#55: w=[-3.426365  -3.4190686  5.3407407], loss=0.4794613718986511\n",
            "#56: w=[-3.4529831 -3.4460943  5.380345 ], loss=0.4734828770160675\n",
            "#57: w=[-3.4792857 -3.472778   5.419475 ], loss=0.4676480293273926\n",
            "#58: w=[-3.5052798 -3.4991288  5.4581428], loss=0.4619514048099518\n",
            "#59: w=[-3.5309722 -3.525155   5.496359 ], loss=0.45638811588287354\n",
            "#60: w=[-3.5563695 -3.5508652  5.534134 ], loss=0.4509538412094116\n",
            "#61: w=[-3.5814784 -3.5762672  5.571478 ], loss=0.4456438422203064\n",
            "#62: w=[-3.6063046 -3.6013687  5.6084   ], loss=0.44045382738113403\n",
            "#63: w=[-3.6308546 -3.626177   5.64491  ], loss=0.43538036942481995\n",
            "#64: w=[-3.6551342 -3.6506991  5.681017 ], loss=0.43041884899139404\n",
            "#65: w=[-3.679149  -3.6749418  5.7167296], loss=0.4255657494068146\n",
            "#66: w=[-3.7029045 -3.6989117  5.752056 ], loss=0.4208178222179413\n",
            "#67: w=[-3.726406  -3.7226148  5.787005 ], loss=0.416171669960022\n",
            "#68: w=[-3.749659  -3.7460573  5.821584 ], loss=0.41162407398223877\n",
            "#69: w=[-3.7726684 -3.7692451  5.855801 ], loss=0.4071716070175171\n",
            "#70: w=[-3.7954388 -3.7921839  5.8896637], loss=0.40281152725219727\n",
            "#71: w=[-3.817975  -3.814879   5.9231787], loss=0.3985413610935211\n",
            "#72: w=[-3.840282  -3.8373356  5.956353 ], loss=0.394357830286026\n",
            "#73: w=[-3.8623638 -3.8595588  5.989194 ], loss=0.39025843143463135\n",
            "#74: w=[-3.8842251 -3.8815534  6.021707 ], loss=0.38624119758605957\n",
            "#75: w=[-3.90587   -3.9033241  6.0539   ], loss=0.38230305910110474\n",
            "#76: w=[-3.9273026 -3.9248757  6.0857773], loss=0.3784417510032654\n",
            "#77: w=[-3.9485269 -3.9462125  6.1173463], loss=0.37465572357177734\n",
            "#78: w=[-3.9695468 -3.9673388  6.1486125], loss=0.37094205617904663\n",
            "#79: w=[-3.990366  -3.9882586  6.1795816], loss=0.36729905009269714\n",
            "#80: w=[-4.010988  -4.0089765  6.210259 ], loss=0.3637247681617737\n",
            "#81: w=[-4.031417  -4.0294957  6.24065  ], loss=0.36021724343299866\n",
            "#82: w=[-4.051656  -4.0498204  6.27076  ], loss=0.3567746877670288\n",
            "#83: w=[-4.071708  -4.069954   6.3005943], loss=0.353395015001297\n",
            "#84: w=[-4.0915775 -4.0899     6.3301573], loss=0.3500770330429077\n",
            "#85: w=[-4.111267  -4.1096625  6.3594537], loss=0.3468186855316162\n",
            "#86: w=[-4.1307797 -4.1292443  6.388489 ], loss=0.3436187505722046\n",
            "#87: w=[-4.1501184 -4.1486487  6.4172664], loss=0.3404753804206848\n",
            "#88: w=[-4.1692863 -4.167879   6.4457912], loss=0.33738696575164795\n",
            "#89: w=[-4.1882863 -4.186939   6.4740677], loss=0.3343527317047119\n",
            "#90: w=[-4.2071214 -4.20583    6.5021   ], loss=0.33137059211730957\n",
            "#91: w=[-4.2257943 -4.2245564  6.5298924], loss=0.32843977212905884\n",
            "#92: w=[-4.2443075 -4.2431207  6.557449 ], loss=0.3255586624145508\n",
            "#93: w=[-4.262664  -4.2615256  6.584773 ], loss=0.32272613048553467\n",
            "#94: w=[-4.2808657 -4.2797737  6.611869 ], loss=0.31994083523750305\n",
            "#95: w=[-4.2989154 -4.297868   6.63874  ], loss=0.31720155477523804\n",
            "#96: w=[-4.316816 -4.31581   6.66539 ], loss=0.3145076334476471\n",
            "#97: w=[-4.3345695 -4.333604   6.6918225], loss=0.3118574321269989\n",
            "#98: w=[-4.352178  -4.3512506  6.718041 ], loss=0.3092502951622009\n",
            "#99: w=[-4.369644  -4.3687534  6.7440486], loss=0.30668509006500244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FyoS5iP6n7Ru",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0cbc76f8-bcfc-4c5f-8a50-c7f2e6d0b783"
      },
      "source": [
        "w"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.2430],\n",
              "        [-4.2432],\n",
              "        [ 6.5565]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jfp604gFn9Yw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "941c1dd1-0304-405f-8944-5df8ffb17c76"
      },
      "source": [
        "x.mm(w).sigmoid()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9986],\n",
              "        [0.9100],\n",
              "        [0.9100],\n",
              "        [0.1268]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w6eu-AuDvl9J"
      },
      "source": [
        "### Multi-layer neural network using automatic differentiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ts2RTKVPn_xk",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "w1 = torch.randn(3, 2, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(2, 1, dtype=dtype, requires_grad=True)\n",
        "b2 = torch.randn(1, 1, dtype=dtype, requires_grad=True)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(1000):\n",
        "    # y_pred = \\sigma(w_2 \\cdot \\sigma(x \\cdot w_1) + b_2)\n",
        "    y_pred = x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()\n",
        "    ll = y * y_pred + (1 - y) * (1 - y_pred)\n",
        "    loss = -ll.log().sum()\n",
        "    loss.backward()\n",
        "\n",
        "    #print(f'#{t}: loss={loss.item()}')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Update weights using SGD.\n",
        "        w1 -= eta * w1.grad\n",
        "        w2 -= eta * w2.grad\n",
        "        b2 -= eta * b2.grad\n",
        "        \n",
        "        # Clear the gradients for the next iteration.\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()\n",
        "        b2.grad.zero_()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FWgbqAXawEof",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "267eb0c0-da06-4c02-e9f3-7356a50cb140"
      },
      "source": [
        "print(w1)\n",
        "print(w2)\n",
        "print(b2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-5.9381,  7.3522],\n",
            "        [-5.9374,  7.3486],\n",
            "        [ 8.8981, -3.3369]], requires_grad=True)\n",
            "tensor([[9.9424],\n",
            "        [9.8626]], requires_grad=True)\n",
            "tensor([[-14.5647]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yS4bql3foxB5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "54fe8fce-fa2b-424b-9b79-19a69042457f"
      },
      "source": [
        "x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0136],\n",
              "        [0.9898],\n",
              "        [0.9898],\n",
              "        [0.0145]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OGS23bDSazMJ"
      },
      "source": [
        "### Single-layer neural network with high-level NN modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jt9eizLFo1iN",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = model(x)                   # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
        "\n",
        "    #print(f'#{t}: loss={loss.item()}')\n",
        "    \n",
        "    model.zero_grad()                   # Zero-clear the gradients.\n",
        "    loss.backward()                     # Compute the gradients.\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= eta * param.grad   # Update the parameters using SGD."
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zq6oqLmIENFa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cb5fe9fd-0e18-4df7-8641-409b771f00a1"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-4.2650, -4.2642]])),\n",
              "             ('0.bias', tensor([6.5885]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QdPOdNgO840b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "07db825e-c669-4265-ea19-07a30082cb56"
      },
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9986],\n",
              "        [0.9109],\n",
              "        [0.9108],\n",
              "        [0.1256]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KfuoJMeqbClA"
      },
      "source": [
        "### Multi-layer neural network with high-level NN modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D6ss25zA9nPk",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n",
        "    torch.nn.Sigmoid(),                 # Sigmoid function\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)                   # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
        "\n",
        "    #print(f'#{t}: loss={loss.item()}')\n",
        "    \n",
        "    model.zero_grad()                   # Zero-clear the gradients.\n",
        "    loss.backward()                     # Compute the gradients.\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= eta * param.grad   # Update the parameters using SGD."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iLFGZWL0--2n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "4e507da2-ca09-40cb-ba10-41caf1f51072"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-0.1494, -0.0430],\n",
              "                      [-0.1074,  0.0421]])),\n",
              "             ('0.bias', tensor([-0.6541, -0.5074])),\n",
              "             ('2.weight', tensor([[-0.0254, -0.0099]])),\n",
              "             ('2.bias', tensor([0.0118]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1BF6-W_J-82M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "129a4047-2454-46b1-a6dd-e974bacd817a"
      },
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4998],\n",
              "        [0.4999],\n",
              "        [0.5001],\n",
              "        [0.5001]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gGK3DJBubb0f"
      },
      "source": [
        "### Single-layer neural network with an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "puqbP4F9bidv",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(100):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "\n",
        "    #print(f'#{t}: loss={loss.item()}')\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RBUX1BUhcDK4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f880eca0-7dc9-4fe9-be5d-a3b5ab55a176"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-4.2643, -4.2638]])),\n",
              "             ('0.bias', tensor([6.5877]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nJWcDaPpcKCB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "77f0b7a3-bb0e-414d-a533-59eb016e46fa"
      },
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9986],\n",
              "        [0.9108],\n",
              "        [0.9108],\n",
              "        [0.1256]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "scIE8zZWdhLs"
      },
      "source": [
        "### Multi-layer neural networks using an optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J1BrWIxkcMfI",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n",
        "    torch.nn.Sigmoid(),                 # Sigmoid function\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "\n",
        "    #print(f'#{t}: loss={loss.item()}')\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mp0sNnxhducs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "4ab06701-cea7-4956-aebe-4d77328a9fa1"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-5.5683, -5.5688],\n",
              "                      [ 7.3365,  7.3404]])),\n",
              "             ('0.bias', tensor([ 8.3403, -3.3190])),\n",
              "             ('2.weight', tensor([[9.4095, 9.2770]])),\n",
              "             ('2.bias', tensor([-13.7123]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AYgEteqydwt2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "8b944d52-eaca-4c65-8446-89874bdae670"
      },
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0183],\n",
              "        [0.9860],\n",
              "        [0.9860],\n",
              "        [0.0200]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7x7Ed3SBGjPx"
      },
      "source": [
        "### Single-layer neural network with a customizable NN class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "klKcvlpVkpk9",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network model.\n",
        "class SingleLayerNN(torch.nn.Module):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super(SingleLayerNN, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(d_in, d_out, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear1(x)\n",
        "\n",
        "model = SingleLayerNN(2, 1)\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(100):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "\n",
        "    #print(f'#{t}: loss={loss.item()}')\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ExxVfDnB5vPW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b26b99f4-21b6-44a2-944d-ac12e0c765d4"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight', tensor([[-4.2721, -4.2722]])),\n",
              "             ('linear1.bias', tensor([6.5998]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xKczD7tZ518W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "3543fd1d-4323-40b2-b33b-bb0988bc5872"
      },
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9986],\n",
              "        [0.9111],\n",
              "        [0.9111],\n",
              "        [0.1251]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jzTPZXUxGNso"
      },
      "source": [
        "### Multi-layer neural network with a customizable NN class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gSpf1qft53-O",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network model.\n",
        "class ThreeLayerNN(torch.nn.Module):\n",
        "    def __init__(self, d_in, d_hidden, d_out):\n",
        "        super(ThreeLayerNN, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(d_in, d_hidden, bias=True)\n",
        "        self.linear2 = torch.nn.Linear(d_hidden, d_out, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.linear1(x).sigmoid())\n",
        "\n",
        "model = ThreeLayerNN(2, 2, 1)\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "\n",
        "    #print(f'#{t}: loss={loss.item()}')\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k6LLbCf0684G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "b85d932d-2892-4ab8-ef2e-8bd1dc04f232"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight', tensor([[ 7.5323, -5.1217],\n",
              "                      [ 6.7442,  2.6708]])),\n",
              "             ('linear1.bias', tensor([-5.8752, -8.3220])),\n",
              "             ('linear2.weight', tensor([[ 7.0725, -6.3992]])),\n",
              "             ('linear2.bias', tensor([-0.0026]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iPhm3J4R61S2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "90c01094-defb-45ec-b24b-80c1fd1f03f7"
      },
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5039],\n",
              "        [0.4938],\n",
              "        [0.9922],\n",
              "        [0.0101]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}