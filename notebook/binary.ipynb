{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/chokkan/deeplearning/blob/master/notebook/binary.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CX_9BuPB_hA-"
   },
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "This Jupyter notebook explains various ways of implementing single-layer and multi-layer neural networks. The implementations are arranged by concrete (explicit) to abstract order so that one can understand the black-boxed processing in deep learning frameworks.\n",
    "\n",
    "In order to focus on understanding the internals of training, this notebook uses a simple and classic example: *threshold logic units*.\n",
    "Supposing $x=0$ as *false* and $x=1$ as *true*, single-layer neural networks can realize logic units such as AND ($\\wedge$), OR ($\\vee$), NOT ($\\lnot$), and NAND ($|$). Multi-layer neural networks can realize logical compounds such as XOR.\n",
    "\n",
    "| $x_1$ | $x_2$ | AND | OR | NAND | XOR |\n",
    "| :---: |:-----:|:---:|:--:|:----:|:---:|\n",
    "| 0 | 0 | 0 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 0 | 1 | 1 | 1 |\n",
    "| 1 | 0 | 0 | 1 | 1 | 1 |\n",
    "| 1 | 1 | 1 | 1 | 0 | 0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxdFG8U2Net8"
   },
   "source": [
    "## Using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ea5cM4JEsENr"
   },
   "source": [
    "### Single-layer perceptron\n",
    "\n",
    "A single layer perceptron predicts a binary label $\\hat{y} \\in \\{0, 1\\}$ for a given input vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ ($d$ presents the number of dimensions of inputs) by using the following formula,\n",
    "$$\n",
    "\\hat{y} = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = g(w_1 x_1 + w_2 x_2 + ... + w_d x_d + b)\n",
    "$$\n",
    "\n",
    "Here, $\\boldsymbol{w} \\in \\mathbb{R}^d$ is a weight vector; $b \\in \\mathbb{R}$ is a bias weight; and $g(.)$ denotes a Heaviside step function (we assume $g(0)=0$).\n",
    "\n",
    "Let's train a NAND gate with two inputs ($d = 2$). More specifically, we want to find a weight vector $\\boldsymbol{w}$ and a bias weight $b$ of a single-layer perceptron that realizes the truth table of the NAND gate: $\\{0,1\\}^2 \\to \\{0,1\\}$.\n",
    "\n",
    "| $x_1$ | $x_2$ | $y$  |\n",
    "| :---: |:-----:|:----:|\n",
    "| 0 | 0 | 1|\n",
    "| 0 | 1 | 1|\n",
    "| 1 | 0 | 1|\n",
    "| 1 | 1 | 0|\n",
    "\n",
    "We convert the truth table into a training set consisting of all mappings of the NAND gate,\n",
    "$$\n",
    "\\boldsymbol{x}_1 = (0, 0), y_1 = 1 \\\\\n",
    "\\boldsymbol{x}_2 = (0, 1), y_2 = 1 \\\\\n",
    "\\boldsymbol{x}_3 = (1, 0), y_3 = 1 \\\\\n",
    "\\boldsymbol{x}_4 = (1, 1), y_4 = 0 \\\\\n",
    "$$\n",
    "\n",
    "In order to train a weight vector and bias weight in a unified code, we include a bias term as an additional dimension to inputs. More concretely, we append $1$ to each input,\n",
    "$$\n",
    "\\boldsymbol{x}'_1 = (0, 0, 1), y_1 = 1 \\\\\n",
    "\\boldsymbol{x}'_2 = (0, 1, 1), y_2 = 1 \\\\\n",
    "\\boldsymbol{x}'_3 = (1, 0, 1), y_3 = 1 \\\\\n",
    "\\boldsymbol{x}'_4 = (1, 1, 1), y_4 = 0 \\\\\n",
    "$$\n",
    "\n",
    "Then, the formula of the single-layer perceptron becomes,\n",
    "$$\n",
    "\\hat{y} = g((w_1, w_2, w_3) \\cdot \\boldsymbol{x}') = g(w_1 x_1 + w_2 x_2 + w_3)\n",
    "$$\n",
    "In other words, $w_1$ and $w_2$ present weights for $x_1$ and $x_2$, respectively, and $w_3$ does a bias weight.\n",
    "\n",
    "The code below implements Rosenblatt's perceptron algorithm with a fixed number of iterations (100 times). We use a constant learning rate 0.5 for simplicity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ygoUjQYrPoj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training data for NAND.\n",
    "x = np.array([\n",
    "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
    "    ])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "w = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "eta = 0.5\n",
    "for t in range(100):\n",
    "    for i in range(len(y)):\n",
    "        y_pred = np.heaviside(np.dot(x[i], w), 0)\n",
    "        w += (y[i] - y_pred) * eta * x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TYoeshu2rXdK",
    "outputId": "5795355a-ce1b-4c7b-dfc9-b7cfcd9ad6c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1. ,  0.5, -1. ])"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hOFgUFojraFA",
    "outputId": "31676a50-61d6-4581-f972-90b1144df906"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.heaviside(np.dot(x, w), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bl_ZAEguNzgU"
   },
   "source": [
    "### Single-layer perceptron with mini-batch\n",
    "\n",
    "It is desireable to reduce the execusion run by the Python interpreter, which is relatively slow. The common technique to speed up a machine-learning code written in Python is to to execute computations within the matrix library (e.g., numpy).\n",
    "\n",
    "The single-layer perceptron makes predictions for four inputs,\n",
    "$$\n",
    "\\hat{y}_1 = g(\\boldsymbol{x}_1 \\cdot \\boldsymbol{w}) \\\\\n",
    "\\hat{y}_2 = g(\\boldsymbol{x}_2 \\cdot \\boldsymbol{w}) \\\\\n",
    "\\hat{y}_3 = g(\\boldsymbol{x}_3 \\cdot \\boldsymbol{w}) \\\\\n",
    "\\hat{y}_4 = g(\\boldsymbol{x}_4 \\cdot \\boldsymbol{w}) \\\\\n",
    "$$\n",
    "\n",
    "Here, we define $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ and $X \\in \\mathbb{R}^{4 \\times d}$ as,\n",
    "$$\n",
    "\\hat{Y} = \\begin{pmatrix} \n",
    "  \\hat{y}_1 \\\\ \n",
    "  \\hat{y}_2 \\\\ \n",
    "  \\hat{y}_3 \\\\ \n",
    "  \\hat{y}_4 \\\\ \n",
    "\\end{pmatrix},\n",
    "X = \\begin{pmatrix} \n",
    "  \\boldsymbol{x}_1 \\\\ \n",
    "  \\boldsymbol{x}_2 \\\\ \n",
    "  \\boldsymbol{x}_3 \\\\ \n",
    "  \\boldsymbol{x}_4 \\\\ \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then, we can write the four predictions in one dot-product computation,\n",
    "$$\n",
    "\\hat{Y} = X \\cdot \\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "The code below implements this idea. The function `np.heaviside()` yields a vector corresponding to the four predictions, applying the step function for every element of the argument.\n",
    "\n",
    "This technique is frequently used in mini-batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fK-_WimtPwb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training data for NAND.\n",
    "x = np.array([\n",
    "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
    "    ])\n",
    "y = np.array([1, 1, 1, 0])\n",
    "w = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "eta = 0.5\n",
    "for t in range(100):\n",
    "    y_pred = np.heaviside(np.dot(x, w), 0)\n",
    "    w += np.dot((y - y_pred), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_x4p1BldtQ-K",
    "outputId": "f01a29f7-d0ec-4e12-9f1d-9ffe6ad99fe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1.,  2.])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "E-P2RpWrtVyf",
    "outputId": "ebc82302-8cc1-4a16-9515-c0970f91c1a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.heaviside(np.dot(x, w), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkxBcCSTtDvm"
   },
   "source": [
    "### Stochastic gradient descent (SGD) with mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bltpfNRctjV5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(v):\n",
    "    return 1.0 / (1 + np.exp(-v))\n",
    "\n",
    "# Training data for NAND.\n",
    "x = np.array([\n",
    "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
    "    ])\n",
    "y = np.array([1, 1, 1, 0])\n",
    "w = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "eta = 0.5\n",
    "for t in range(100):\n",
    "    y_pred = sigmoid(np.dot(x, w))\n",
    "    w -= np.dot((y_pred - y), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9bASDMfhtm-I",
    "outputId": "eeb3bf87-4a94-4189-c0f4-f3ebdb5005a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.59504346, -5.59504346,  8.57206068])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-69r0c4KtqlW",
    "outputId": "65263ac7-b37e-496b-c651-47de7e3b5b8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99981071, 0.95152498, 0.95152498, 0.06798725])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELUeNFRFuJv3"
   },
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aB0DwVOyuXP_"
   },
   "source": [
    "### Using autograd\n",
    "\n",
    "Installing [autograd](https://github.com/HIPS/autograd) (do this once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "kpt7DJ7a-qov",
    "outputId": "5d0d3f7b-8852-4dc3-dc74-1e9768595df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autograd in /usr/local/lib/python3.6/dist-packages (1.2)\r\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\r\n",
      "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "DyC9iOFO-1cd",
    "outputId": "89aa3a6a-f865-477a-f4d9-c6f4f0a572f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47407698418010663\n",
      "[-0.37754067 -0.37754067 -0.37754067]\n"
     ]
    }
   ],
   "source": [
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "def loss(w, x):\n",
    "    return -np.log(1.0 / (1 + np.exp(-np.dot(x, w))))\n",
    "\n",
    "x = np.array([1, 1, 1])\n",
    "w = np.array([1.0, 1.0, -1.5])\n",
    "\n",
    "grad_loss = autograd.grad(loss)\n",
    "print(loss(w, x))\n",
    "print(grad_loss(w, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59D2aARTuQDL"
   },
   "source": [
    "### Using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "VOoZXvXP-6b2",
    "outputId": "d184e47f-ec66-4c78-e777-d928ac22e5a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.0)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "nHixTUut_LIm",
    "outputId": "30d28589-4e9f-451c-ab36-d76db7887149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4740769565105438\n",
      "tensor([-0.3775, -0.3775, -0.3775])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "x = torch.tensor([1, 1, 1], dtype=dtype)\n",
    "w = torch.tensor([1.0, 1.0, -1.5], dtype=dtype, requires_grad=True)\n",
    "\n",
    "loss = -torch.dot(x, w).sigmoid().log()\n",
    "loss.backward()\n",
    "print(loss.item())\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFzuau5gu4vY"
   },
   "source": [
    "## Implementing neural networks with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STwWdvJCva4G"
   },
   "source": [
    "### Single-layer neural network using automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOJHDGYKIgsm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Training data for NAND.\n",
    "x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n",
    "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
    "w = torch.randn(3, 1, dtype=dtype, requires_grad=True)\n",
    "\n",
    "eta = 0.5\n",
    "for t in range(100):\n",
    "    # y_pred = \\sigma(x \\cdot w)\n",
    "    y_pred = x.mm(w).sigmoid()\n",
    "    ll = y * y_pred + (1 - y) * (1 - y_pred)\n",
    "    loss = -ll.log().sum()      # The loss value.\n",
    "    #print(t, loss.item())\n",
    "    loss.backward()             # Compute the gradients of the loss.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= eta * w.grad       # Update weights using SGD.        \n",
    "        w.grad.zero_()          # Clear the gradients for the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "FyoS5iP6n7Ru",
    "outputId": "fe6de935-5f28-4a67-9f97-b8447e138f2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.2228],\n",
       "        [-4.2234],\n",
       "        [ 6.5268]])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Jfp604gFn9Yw",
    "outputId": "27fdd3f8-8607-4076-a962-0ac38618f8d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9985],\n",
       "        [ 0.9092],\n",
       "        [ 0.9092],\n",
       "        [ 0.1279]])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mm(w).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6eu-AuDvl9J"
   },
   "source": [
    "### Multi-layer neural network using automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ts2RTKVPn_xk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Training data for XOR.\n",
    "x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
    "w1 = torch.randn(3, 2, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(2, 1, dtype=dtype, requires_grad=True)\n",
    "b2 = torch.randn(1, 1, dtype=dtype, requires_grad=True)\n",
    "\n",
    "eta = 0.5\n",
    "for t in range(1000):\n",
    "    # y_pred = \\sigma(w_2 \\cdot \\sigma(x \\cdot w_1) + b_2)\n",
    "    y_pred = x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()\n",
    "    ll = y * y_pred + (1 - y) * (1 - y_pred)\n",
    "    loss = -ll.log().sum()\n",
    "    #print(t, loss.item())\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Update weights using SGD.\n",
    "        w1 -= eta * w1.grad\n",
    "        w2 -= eta * w2.grad\n",
    "        b2 -= eta * b2.grad\n",
    "        \n",
    "        # Clear the gradients for the next iteration.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        b2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "FWgbqAXawEof",
    "outputId": "c1b87740-d7b4-4ba3-f327-81bf56199850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.4129, -7.0510],\n",
      "        [-5.4112, -7.0391],\n",
      "        [ 8.0656,  2.9682]])\n",
      "tensor([[ 11.6259],\n",
      "        [-12.0312]])\n",
      "tensor([[-5.4634]])\n"
     ]
    }
   ],
   "source": [
    "print(w1)\n",
    "print(w2)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "yS4bql3foxB5",
    "outputId": "dadf8c7c-429e-426a-d9f4-53bf8e0fa4c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0050],\n",
       "        [ 0.9945],\n",
       "        [ 0.9945],\n",
       "        [ 0.0084]])"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGS23bDSazMJ"
   },
   "source": [
    "### Single-layer neural network with high-level NN modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jt9eizLFo1iN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Training data for NAND.\n",
    "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
    "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
    "                                        \n",
    "# Define a neural network using high-level modules.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
    ")\n",
    "\n",
    "# Binary corss-entropy loss after sigmoid function.\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
    "\n",
    "eta = 0.5\n",
    "for t in range(100):\n",
    "    y_pred = model(x)                   # Make predictions.\n",
    "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
    "    #print(t, loss.item())\n",
    "    \n",
    "    model.zero_grad()                   # Zero-clear the gradients.\n",
    "    loss.backward()                     # Compute the gradients.\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= eta * param.grad   # Update the parameters using SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Zq6oqLmIENFa",
    "outputId": "83ddfa07-461e-4746-8669-6a74d6bdd3d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[-4.2632, -4.2634]])),\n",
       "             ('0.bias', tensor([ 6.5866]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "QdPOdNgO840b",
    "outputId": "afee61a6-608d-4ba0-9cac-e09947421f8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9986],\n",
       "        [ 0.9108],\n",
       "        [ 0.9108],\n",
       "        [ 0.1256]])"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfuoJMeqbClA"
   },
   "source": [
    "### Multi-layer neural network with high-level NN modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6ss25zA9nPk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Training data for XOR.\n",
    "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
    "                                        \n",
    "# Define a neural network using high-level modules.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n",
    "    torch.nn.Sigmoid(),                 # Sigmoid function\n",
    "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
    ")\n",
    "\n",
    "# Binary corss-entropy loss after sigmoid function.\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
    "\n",
    "eta = 0.5\n",
    "for t in range(1000):\n",
    "    y_pred = model(x)                   # Make predictions.\n",
    "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
    "    #print(t, loss.item())\n",
    "    \n",
    "    model.zero_grad()                   # Zero-clear the gradients.\n",
    "    loss.backward()                     # Compute the gradients.\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= eta * param.grad   # Update the parameters using SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "iLFGZWL0--2n",
    "outputId": "0f2c7a0f-c31e-486b-994b-8d91f20e0835"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[-5.3436, -5.3484],\n",
       "                      [-6.9310, -6.9626]])),\n",
       "             ('0.bias', tensor([ 7.9648,  2.9143])),\n",
       "             ('2.weight', tensor([[ 11.4116, -11.8373]])),\n",
       "             ('2.bias', tensor([-5.3505]))])"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "1BF6-W_J-82M",
    "outputId": "f6d54774-9eb7-49eb-a6cc-8fd3fc8cae4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0056],\n",
       "        [ 0.9938],\n",
       "        [ 0.9938],\n",
       "        [ 0.0095]])"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGK3DJBubb0f"
   },
   "source": [
    "### Single-layer neural network with an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puqbP4F9bidv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Training data for NAND.\n",
    "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
    "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
    "                                        \n",
    "# Define a neural network using high-level modules.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
    ")\n",
    "\n",
    "# Binary corss-entropy loss after sigmoid function.\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
    "\n",
    "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "for t in range(100):\n",
    "    y_pred = model(x)           # Make predictions.\n",
    "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
    "    #print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()       # Zero-clear gradients.\n",
    "    loss.backward()             # Compute the gradients.\n",
    "    optimizer.step()            # Update the parameters using the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "RBUX1BUhcDK4",
    "outputId": "99f6202e-af36-49b8-85e9-aca6d08d13b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[-4.2689, -4.2692]])),\n",
       "             ('0.bias', tensor([ 6.5951]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "nJWcDaPpcKCB",
    "outputId": "109c780d-03a7-4dfe-aa75-30aa619e05e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9986],\n",
       "        [ 0.9110],\n",
       "        [ 0.9110],\n",
       "        [ 0.1253]])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scIE8zZWdhLs"
   },
   "source": [
    "### Multi-layer neural networks using an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J1BrWIxkcMfI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Training data for XOR.\n",
    "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
    "                                        \n",
    "# Define a neural network using high-level modules.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n",
    "    torch.nn.Sigmoid(),                 # Sigmoid function\n",
    "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
    ")\n",
    "\n",
    "# Binary corss-entropy loss after sigmoid function.\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
    "\n",
    "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "for t in range(1000):\n",
    "    y_pred = model(x)           # Make predictions.\n",
    "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
    "    #print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()       # Zero-clear gradients.\n",
    "    loss.backward()             # Compute the gradients.\n",
    "    optimizer.step()            # Update the parameters using the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "mp0sNnxhducs",
    "outputId": "eb32ebb1-c216-4858-ad49-405349d6e4e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[-5.8059, -5.8059],\n",
       "                      [ 7.3491,  7.3492]])),\n",
       "             ('0.bias', tensor([ 8.6991, -3.3325])),\n",
       "             ('2.weight', tensor([[ 9.7696,  9.6699]])),\n",
       "             ('2.bias', tensor([-14.2857]))])"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "AYgEteqydwt2",
    "outputId": "91fab3bd-da9a-4e28-d2d1-d5244ca4eadb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0150],\n",
       "        [ 0.9887],\n",
       "        [ 0.9887],\n",
       "        [ 0.0161]])"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7x7Ed3SBGjPx"
   },
   "source": [
    "### Single-layer neural network with a customizable NN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "klKcvlpVkpk9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Training data for NAND.\n",
    "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
    "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
    "                                        \n",
    "# Define a neural network model.\n",
    "class SingleLayerNN(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(d_in, d_out, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear1(x)\n",
    "\n",
    "model = SingleLayerNN(2, 1)\n",
    "\n",
    "# Binary corss-entropy loss after sigmoid function.\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
    "\n",
    "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "for t in range(100):\n",
    "    y_pred = model(x)           # Make predictions.\n",
    "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
    "    #print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()       # Zero-clear gradients.\n",
    "    loss.backward()             # Compute the gradients.\n",
    "    optimizer.step()            # Update the parameters using the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ExxVfDnB5vPW",
    "outputId": "646b257f-8f9b-4a7b-b5e4-398135dce060"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight', tensor([[-4.3039, -4.3036]])),\n",
       "             ('linear1.bias', tensor([ 6.6467]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "xKczD7tZ518W",
    "outputId": "b6892294-be8e-4148-deae-98ce2544cc3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9987],\n",
       "        [ 0.9124],\n",
       "        [ 0.9124],\n",
       "        [ 0.1234]])"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzTPZXUxGNso"
   },
   "source": [
    "### Multi-layer neural network with a customizable NN class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSpf1qft53-O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Training data for XOR.\n",
    "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
    "                                        \n",
    "# Define a neural network model.\n",
    "class ThreeLayerNN(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super(ThreeLayerNN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(d_in, d_hidden, bias=True)\n",
    "        self.linear2 = torch.nn.Linear(d_hidden, d_out, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.linear1(x).sigmoid())\n",
    "\n",
    "model = ThreeLayerNN(2, 2, 1)\n",
    "\n",
    "# Binary corss-entropy loss after sigmoid function.\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
    "\n",
    "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "for t in range(1000):\n",
    "    y_pred = model(x)           # Make predictions.\n",
    "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
    "    #print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()       # Zero-clear gradients.\n",
    "    loss.backward()             # Compute the gradients.\n",
    "    optimizer.step()            # Update the parameters using the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "k6LLbCf0684G",
    "outputId": "f7ae4123-1f79-467e-bb46-6635ba0ea628"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight', tensor([[-6.7532,  6.5182],\n",
       "                      [ 6.3477, -6.6279]])),\n",
       "             ('linear1.bias', tensor([-3.5757, -3.4740])),\n",
       "             ('linear2.weight', tensor([[ 11.2248,  11.2577]])),\n",
       "             ('linear2.bias', tensor([-5.5168]))])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "iPhm3J4R61S2",
    "outputId": "7fcda9bc-ac68-45bd-bbb6-f2193e383451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0076],\n",
       "        [ 0.9942],\n",
       "        [ 0.9942],\n",
       "        [ 0.0066]])"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeOa6mt5Fgw4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "mlp_binary.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
